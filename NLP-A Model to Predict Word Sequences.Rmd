---
title: 'Natural Language Processing: A Model to Predict a Sequence of Words'
author: "Gerald Gendron"
date: "October 13, 2014"
output:
  html_document:
    toc: yes
---

# Executive Summary

This report provides documentation describing the process and decisions used to develop a predictive text model for the Data Science Capstone project. All code used in the development of the project is contained in this report. The main body of the report provides the essential discussion of the product development story by summarizing key aspects of the analysis and model building. Key learning points are included to aid readers interested in reproducing this work and enhancing it. Overall, the discipline of Natural Language Processing is a broad and useful domain of data science. This report includes a brief literature review capturing key concepts that guided this project. The corpus used in this analysis might be considered to have a personality – specifically, that it is a unique collection of words, phrases, and sentences that has been characterized by some exploratory analysis to become familiar with what the corpus is and how it may be used for prediction. The initial model created from a small subset of the corpus was useful but did not enable scalability. Additional research, creative thinking, and persistent modeling alterations resulted in a predictive text model that balanced accuracy with scalability. The model described in this report was ultimately hosted as a web-based application for consumer use and allows some customization of results by users. 

*Keywords: natural language processing, text mining, predictive text analytics, N-Gram, Good-Turing Smoothing, Katz back off*

``` {r 0-createDataSubsets, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}

##################################
##################################
## For RMD purposes, please note that the entire body of code is place within the document to allow for runs of graphics. You will notice that most front "Chunks" are set to echo=FALSE and eval=FALSE. This is done to enable generation of the PDF report
##################################
##################################


##
## Create data subsets from original datasets provided
##

###
## Note: prior to running, create these three folders and extract the associated data into each - one corpus per folder:
# Folder Names: blogs, news, twitter
###

library(tm) # Use tm package
# Read in three English text data sets already extracted from .zip file
# Used virtual corpus for this interim work
tweet <- VCorpus(DirSource("twitter", encoding = "UTF-8"), 
                 readerControl=list(language="en"))
blog <- VCorpus(DirSource("blogs", encoding = "UTF-8"),
                readerControl=list(language="en"))
news <- VCorpus(DirSource("news", encoding = "UTF-8"),
                readerControl=list(language="en"))
########
## Approach to split corpora into train, devtest, and test sets
########

# Permute entire tweet group to randomize order
set.seed(0330)
perm.tweet <- sample(tweet[[2]][[1]], length(tweet[[2]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.tweet))
twtTrain <- perm.tweet[1:TR]
remain<-perm.tweet[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
twtDevtest <- remain[1:DEV]
twtTest <- remain[-(1:DEV)]
write(twtTrain,"twtTrain.txt")
write(twtDevtest,"twtDevtest.txt")
write(twtTest,"twtTest.txt")
rm(list = ls()) #clean environment

# Permute entire blog group to randomize order                     
set.seed(0330)
perm.blog <- sample(blog[[1]][[1]], length(blog[[1]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.blog))
blogTrain <- perm.blog[1:TR]
remain<-perm.blog[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
blogDevtest <- remain[1:DEV]
blogTest <- remain[-(1:DEV)]
write(blogTrain,"blogTrain.txt")
write(blogDevtest,"blogDevtest.txt")
write(blogTest,"blogTest.txt")
rm(list = ls()) #clean environment

# Permute entire news group to randomize order   
set.seed(0330)
perm.news <- sample(news[[1]][[1]], length(news[[1]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.news))
newsTrain <- perm.news[1:TR]
remain<-perm.news[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
newsDevtest <- remain[1:DEV]
newsTest <- remain[-(1:DEV)]
write(newsTrain,"newsTrain.txt")
write(newsDevtest,"newsDevtest.txt")
write(newsTest,"newsTest.txt")
rm(list = ls()) #clean environment
```

# Understanding the Problem

A most important aspect at the outset of a data analysis project is to understand the problem. With the advent of social media and blogs, the information value of raw text continues to increase. The problem that exists is in analyzing a large corpus of text to discover the structure and arrangement of words within the data in order to analyze the corpus using computational methods. 
The essence of this project is to take a corpus (a bodey) of text from various sources, clean and analyze that text data, and build a predictive model to present the next likely word in a stream of text provided by a user. User input could range from formal, professional communication to informal, short messages - such as social media. Therefore, knowledge of the data sources in the corpus is essential. As a concrete example, a user may type into their mobile device - "I would like to". A predictive text model would present the most likely options for what the next word might be such as *"eat"*, *"go"*, or *"have"* - to name a few.

Data sciences are increasingly making use of Natural Language Processing combined with statistical methods developed within the arts and humanities decades ago to characterize and leverage the streams of data that are text based and not inherently quantitative. There are many techniques available within the R programming language to work with text to work with them quantitatively. A key aspect of this project is to discern which techniques best promote accuracy and scalability for large data sets.

# Literature Review

*A brief history of text analysis*

Alan Turing (1950) opens his influential article "Computing Machinery and Intelligence" with the statment, "I propose to consider the question, 'Can machines think?'" (p. 433). He follows this by outlining something he calls the imagination game played by man A - known as label X, woman B - known as label Y, and interrogator C. The interrogator is able to ask questions of X or Y in order to attain the objective to properly identify whether "X is A and Y is B" (p. 433) or vice versa. Turing later refines the original question to read, "Are there imaginable digital computers which would do well in the imitation game?" (p. 442). In essence - to predict truth based on features or interrogation. He speaks to a machine teaching process made up of rewards and punishments enforcing orders in symbolic language. Overall, this Turing Test has become a basis of natural language processing - covering a broad array of uses such as spelling correction, speech recognition, author identification, and prediction of words based on preceding words. 

*Literature review purpose and findings*

At the outset of this project, course instructors provided us with various sources on natural language processing, text mining, and various R programming packages they felt would be useful. Further literature review was conducted in order to understand the field and extract clues for building a model. Primary goals of the literature review were to understand:

* common issues when analyzing text data
* the body of knowledge that has built up in the domain of natural language processing
* other resources not provided by this course for helping us in the analysis methodology

This section is not a comprehensive overview of over 40 sources reviewed but merely a summary of two works most influential in shaping the modeling approach taken in this project. 

Course notes for the Capstone project invited participants to become familiar with the R packages on NLP (openNLP) and text mining (tm). Feinerer, Hornik, and Meyer (2008) provide a good overview of the essential aspects of these packages. Noteworthy was their information about: reading in corpora into the R environment, explaining functions to transform the data, explaining stemming, stopwords, and tagging parts of speech, considering the issue of text sparsity, and understanding the fundamental of count based analysis. 

Jurafsky and Martin (2000) provide a seminal work within the domain of NLP. The authors present a key approach for building prediction models called the N-Gram, which relies on knowledge of word sequences from *(N – 1)* prior words. It is a type of language model based on counting words in the corpora to establish probabilities about next words. Overall, Jurafsky and Martin's work had the greatest influence on this project in choosing among many possible strategies for developing a model to predict word selection. It addresses multiple perspectives of the topics found in Feinerer, Hornik, and Meyer. The following approaches and assumptions were chosen For purposes of building an initial model. They may be adapted as models are refined to increase prediction accuracy. 

1. Case: corpora words will not be case-sensitive. Although important for spelling correction and part of speech analysis, the words themselves - not their case - are important for prediction. 
2. Stopwords: similarly, unlike classification and clustering applications, all words will be included in the model as they represent more than just the primary carriers of the message. 
3. Wordform: stemming will not be used as N-Grams are typically based on wordforms (unique, inflected forms of words). So whereas *table* and *tables* are the same lemma, they will be treated as separate words in this model. 
4. Punctuation: Jurafsky and Martin treat punctuation as a word and counted as a word. Given the nature of the SwiftKey approach, which is not trying to generate full sentences but only predict a next word, punctuation will be treated slightly differently in the initial model. End of sentence punctuation (like ? ' ! .) will be used to include end-of-sentence <EOS> tags as the intuition is they have implications for word prediction. 
5. Parts of Speech: the discussion of N-Grams did not imply the inherent value of predication based on syntactically using parts of speech.
6. Numbers: there is no intuition based on the research that numbers will have a great impact on a predication model and they will be removed 
7. Sparse Words: all words will be retained. A key concept from Jurafsky and Martin is the idea that even bigram models are quite sparse; however, rather than eliminating those wordforms, they become clues to the "probability of unseen N-Grams" (p. 209, 2000). They include as their fourth of eight key concepts *Things Seen Once* and recommend using the count of wordforms seen a single time to provide a basis to estimate those things not seen in the training set and will likely appear in a test set 
8. Whitespace: this was not discussed directly by Jurafsky and Martin. The intuition is that whitespace has little to do with context and excess whitespace will be removed

In addition to shaping the initial strategy, the work by Jurafsky and Martin provided valuable insights on other aspects of implementing a predication model.

* Historical context of the unsmoothed N-Gram and basis of probabilistic approach to predicting words using the Markov assumption to simplify probabilities by looking only at (n-1) or (n-2) previous words 
* Importance of having a diverse corpora in order to improve generalizability of prediction among other corpora 
* Comparative analysis of smoothing and discounting techniques to increase predictive power 
*Introduction of backoff techniques to establish probabilities for otherwise unseen elements of an N-Gram 
* Discussion of entropy and perplexity - which may prove to be a good choice as the single measure to help assess quality of the prediction model

# Data Processing

We have to understand the data, determine what should be done with the data, and generate the questions that need to be asked to ascertain whether the data is sufficient to do the job.This section briefly addresses the acquisition, processing, and exploration of the data.

*Data Acquisition and Cleaning*

SwiftKey is the corporate partner involved with this project. They produce software to aid users in rapidly entering text with higher accuracy on mobile devices. Based on their collaboration with Johns Hopkins University, a [data set](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) was provided to the data scientists on this project. The dataset is a zip files including blog posts, news articles, and Twitter tweets in four languages (English, German, Finnish, and Russian). The data was downloaded using the R programming language (R Core Team, 2014) and the elements were extracted using the R text-mining package called `tm` (Feinerer. Hornik, & Artifex Software, 2014). The `PCorpus` function was used as it establishes a permanent database instead of a virtual one. This allowed a database to hold over 3.3 million documents in physical disk memory rather than completely in RAM to reserve processing capacity. <a href="#appendix-a---generating-training-and-testing-sets">Appendix A - Generating 
Training and Testing Sets</a> provides the code and `seeds` used to generate the training and testing datasets.

``` {r 1a-readTraininSettoClean, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Read in dataset and clean for use. The DirSource "training" is a pointer to the location of the training corpus created
##
library(tm)   # package used to read dataset
library(filehash) # package used to update/maintain permanent corpus database
myCorpus <- PCorpus(DirSource("training", encoding="UTF-8",mode="text"),dbControl=list(dbName="myCorpus.db", dbType="DB1"))
```

The data set was cleaned using over 20 transformations that pre-processed the data for analysis. <a href="#appendix-b---reading-in-the-data-and-cleaning">Appendix B - Reading in the Data and Cleaning</a> fully presents each of the transformations in detail. In general, those transformations included: conversion to lower case, ensuring apostrophes were retained to maintain contractions, include end-of-sentence <EOS> and number <NUM> tags, conversion of various ASCII codes to appropriate language wordforms, remove punctuation except apostrophe and <> symbols in <EOS> and <NUM>, remove web URLS, remove numbers, and remove excess whitespace. Intermittently in the process, the corpus was written back to disk and the database was re-initialized using the `filehash` package to reduce the size of data processing in RAM. Although one third of the way through processing the corpus had exceeded 1 GB in RAM, use of `filehash` kept this well below that. The final corpus of over 539,573 blog documents after all processing only 108 MB. It is worth noting the original data set of 899,288 blogs was over 205 MB.

``` {r 1b-readTraininSettoClean, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Cleaning steps refined  over time
## Accomplished in three loops due to RAM limitations
##

# to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower));dbInit("myCorpus.db")

for (j in seq(myCorpus)) {
     # first two separate hyphenated and slashed words
     myCorpus[[j]][[1]] <-gsub("-", " ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("/", " ", myCorpus[[j]][[1]])
     # converts symbol <> into an apostrophe
     myCorpus[[j]][[1]] <-gsub("<>", "\\'", myCorpus[[j]][[1]])
     print("3 of 18 transformations complete") #provides progress to user
     # these three create end of sentence markers <EOS>
     myCorpus[[j]][[1]] <-gsub("\\. |\\.$","  <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\? |\\?$","  <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\! |\\!$","  <EOS> ", myCorpus[[j]][[1]])
     print("6 of 18 transformations complete") #provides progress to user
}
write(myCorpus[[1]][[1]],"./mod/blogTrain.txt") # writes corpus to permanent disc
# reads back in corpus...this method reduced 908MB corpus to 137MB
myCorpus <- PCorpus(DirSource("mod", 
     encoding="UTF-8",mode="text"),dbControl=list(dbName="halfCorpus.db", dbType="DB1"))

for (j in seq(myCorpus)) {
     # tranforms various ASCII codes to appropriate language 
     myCorpus[[j]][[1]] <-gsub("<85>"," <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("<92>","'", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\&", " and ", myCorpus[[j]][[1]])
     print("9 of 18 transformations complete") #provides progress to user
     # removes all punctuation except apostrophe and <> symbols in <EOS>
     myCorpus[[j]][[1]] <-gsub("[^[:alnum:][:space:]\'<>]", " ", myCorpus[[j]][[1]])
     # removes web site URLs
     myCorpus[[j]][[1]] <-gsub(" www(.+) ", " ", myCorpus[[j]][[1]])
     # removes all single letters except "a" and "i"
     myCorpus[[j]][[1]] <-gsub(" [b-hj-z] "," ", myCorpus[[j]][[1]])
     print("12 of 18 transformations complete") #provides progress to user
}

write(myCorpus[[1]][[1]],"./mod/blogTrain.txt")  # writes corpus to permanent disc
# reduced 937 MB corpus to 133 MB
myCorpus <- PCorpus(DirSource("mod",
     encoding="UTF-8",mode="text"),dbControl=list(dbName="lastCorpus.db", dbType="DB1"))

for (j in seq(myCorpus)) {
     # removes errant apostrohes introduced by transformations
     myCorpus[[j]][[1]] <-gsub(" ' "," ", myCorpus[[j]][[1]])        
     myCorpus[[j]][[1]] <-gsub("\\' ", " ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub(" ' ", " ", myCorpus[[j]][[1]])
     print("15 of 18 transformations complete") #provides progress to user
     # removes errant codes in < > brackets
     myCorpus[[j]][[1]] <-gsub("<[^EOS].+>"," ", myCorpus[[j]][[1]])
     # replaces numbers with a number marker <NUM> for context
     myCorpus[[j]][[1]] <-gsub("[0-9]+"," <NUM> ", myCorpus[[j]][[1]])
     # removes and errant <> brackets remaining
     myCorpus[[j]][[1]] <-gsub("<>"," ", myCorpus[[j]][[1]])
     print("18 of 18 transformations complete") #provides progress to user
}
# removes numbers and the tm package "dbInit" compresses data in RAM
myCorpus <- tm_map(myCorpus, removeNumbers);dbInit("lastCorpus.db")
# removes errant 's symbols not as contractions
myCorpus[[1]][[1]] <-gsub(" 's"," ", myCorpus[[1]][[1]])
# removes errant close brackets starting a word
myCorpus[[1]][[1]] <-gsub(">[a-z]"," ", myCorpus[[1]][[1]])

myCorpus <- tm_map(myCorpus, stripWhitespace);dbInit("lastCorpus.db")
# final corpus after all processing only 111 MB

# Writes final, processed corpus to disc for building n-grams
write(myCorpus[[1]][[1]],"./mod/blogTrain.txt") # write PCorpus to disc
# original training set 130 MB; clean training seet 108 MB
```





*Exploratory Analysis*

``` {r 2-buildOneGramModel, echo=FALSE, eval=TRUE,warning=FALSE,error=FALSE,cache=TRUE,message=FALSE}
##
## Code uses blogTrain.txt to generate list of all 1-gram (unigrams)
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="aggCorpus.db", dbType="DB1"))

library(tau) #package to create N-Grams
ngram <- function(n) {
     textcnt(CORP, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]]) #pulls out the text element from the list myCorpus

oneGram <- ngram(1)
unigramDF<-data.frame(Uni = names(oneGram), counts = unclass(oneGram))
rm(oneGram)
unigramDF$Uni<-as.character(unigramDF$Uni)
unigramDF$counts<-as.numeric(unigramDF$counts)
# removes the "words" <eos> and <num> from unigram table
unigramDF <- unigramDF[which(unigramDF$Uni!="<eos>"),]
unigramDF <- unigramDF[which(unigramDF$Uni!="<num>"),]

lengthUni<-length(unigramDF$Uni) #253,921 unigrams

# Builds frequency of frequency table for Good-Turing smoothing
uni.freqfreq<-data.frame(Uni=table(unigramDF$counts))

write.csv(unigramDF,"unigramDF.csv") #2,620 frequencies
write.csv(uni.freqfreq,"uni.freqfreq.csv")
rm(unigramDF,uni.freqfreq,CORP,myCorpus)
```

``` {r 3-buildTwoGramModel, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Code uses blogTrain.txt to generate list of all 2-grams (bigrams)
## Specifically designed to break large corpus into chunks to deal
## with limitations imposed by RAM in R
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="biCorpus.db", dbType="DB1"))
library(tau) #package to create N-Grams
ngram <- function(n) {
     textcnt(CORPport, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]]) #pulls out the text element from the list myCorpus
rm(myCorpus) #reduce RAM load
# determine number of loop runs to process 10,000 docs per run
step<-trunc(length(CORP)/10000)
remain<- length(CORP)-(step * 10000)
# first deal with the remainder
CORPport<-CORP[1:remain]
# invokes tau package to build the bigrams
twoGram <- ngram(2)
names(twoGram) <- gsub("^\'","",names(twoGram))        
twogramDF<-data.frame(Bi = names(twoGram), counts = unclass(twoGram))
names(twogramDF)<-c("Bi","counts")
## removes the "words" <eos> and <num> from unigram table
eosTag<-grepl("<eos>",twogramDF$Bi)
twogramDF<-twogramDF[!eosTag,]
numTag<-grepl("<num>",twogramDF$Bi)
twogramDF<-twogramDF[!numTag,]
# writes first of N:n=step dataframes
write.csv (twogramDF,"twoGramDF1.csv")
CORP<-CORP[-(1:remain)] #remove already processed docs from corpus

for (i in 1:(step-1)) {  # loop to process steps of 10000 docs
     CORPport<-CORP[1:10000]
     twoGram <- ngram(2)
     names(twoGram) <- gsub("^\'","",names(twoGram))        
     tmptwogramDF<-data.frame(Bi = names(twoGram), counts = unclass(twoGram))
     print(paste("Iteration",i,"of",step)) #provide user progress
     name<-paste("twoGramDF",(i+1),".csv",sep="")
     ## removes the "words" <eos> and <num> from unigram table
     eosTag<-grepl("<eos>",tmptwogramDF$Bi)
     tmptwogramDF<-tmptwogramDF[!eosTag,]
     numTag<-grepl("<num>",tmptwogramDF$Bi)
     tmptwogramDF<-tmptwogramDF[!numTag,]
     write.csv (tmptwogramDF,name)
     twogramDF<-rbind(twogramDF,tmptwogramDF)
     twogramDF<-aggregate(twogramDF$counts,list(Bi=twogramDF$Bi),sum)
     names(twogramDF)<-c("Bi","counts")
     CORP<-CORP[-(1:10000)]
}
rm(tmptwogramDF,numTag,eosTag,ngram,CORPport,twoGram)
rm(remain,step,name,i,CORP)
twogramDF$Bi<-as.character(twogramDF$Bi)
twogramDF$counts<-as.numeric(twogramDF$counts)
twogramDF$Uni<-sub(" .*","",twogramDF$Bi)
# Builds frequency of frequency table for Good-Turing smoothing
bi.freqfreq<-data.frame(Bi=table(twogramDF$counts))
write.csv(twogramDF,"twogramDF.csv")
write.csv(bi.freqfreq,"bi-freqfreq.csv")
rm(bi.freqfreq,twogramDF)
```

``` {r 4-buildThreeGramModel, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Code uses blogTrain.txt to generate list of all 3-grams (trigrams)
## Specifically designed to break large corpus into chunks to deal
## with limitations imposed by RAM in R
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="triCorpus.db", dbType="DB1"))
library(tau)
ngram <- function(n) {
     textcnt(CORPport, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]])
rm(myCorpus) #reduce RAM load
# determine number of loop runs to process 10,000 docs per run
step<-trunc(length(CORP)/10000)
for (i in 1:(step)) {
     CORPport<-CORP[1:10000]
     # invokes tau package to build the trigrams
     threeGram <- ngram(3)
     names(threeGram) <- gsub("^\'","",names(threeGram))
     threegramDF<-data.frame(Tri = names(threeGram), counts = unclass(threeGram))
     print(paste("Iteration",i,sep=" "))
     name<-paste("threeGramDF",i,".csv",sep="")
     ## removes the "words" <eos> and <num> from trigram table
     eosTag<-grepl("<eos>",threegramDF$Tri)
     threegramDF<-threegramDF[!eosTag,]
     numTag<-grepl("<num>",threegramDF$Tri)
     threegramDF<-threegramDF[!numTag,]
     # writes first of N:n=step dataframes
     write.csv (threegramDF,name)
     CORP<-CORP[-(1:10000)]
}
CORPport<-CORP
# repeats above loop for the remaining documents (partial step)
threeGram <- ngram(3)
names(threeGram) <- gsub("^\'","",names(threeGram))
threegramDF<-data.frame(Tri = names(threeGram), counts = unclass(threeGram))
names(threegramDF)<-c("Tri","counts")
name<-paste("threeGramDF",i+1,".csv",sep="")
eosTag<-grepl("<eos>",threegramDF$Tri)
threegramDF<-threegramDF[!eosTag,]
numTag<-grepl("<num>",threegramDF$Tri)
threegramDF<-threegramDF[!numTag,]
write.csv (threegramDF,name)

rm(threegramDF, CORP,CORPport,threeGram,i,name)

#####
## Begin aggregating dataframes in groups to develop one trigram dataframe
#####

for (k in seq(1,46,5)) {
     key<-paste("threeGramDF",k,".csv",sep="")
     print(k)
     threeGramDF <-read.csv(file = key)
     threeGramDF<-threeGramDF[,-1]
     for (m in 1:4) {
          name<-paste("threeGramDF",(k+m),".csv",sep="")
          temp <- read.csv(name)
          temp<-temp[,-1]
          threeGramDF <- merge(threeGramDF,temp, by.x="Tri",by.y="Tri", all=TRUE)
          print(k+m)
          rm(temp)
     }
     out<-paste("mergedTri",k,".csv",sep="")
     counts<-threeGramDF[,2:ncol(threeGramDF)]
     threeGramDF$counts<-rowSums(counts,na.rm=TRUE)
     threeGramDF<-threeGramDF[,-(2:(ncol(threeGramDF)-1))]
     write.csv(threeGramDF,out)
     rm(threeGramDF,counts)
}

k=k+5

key<-paste("threeGramDF",k,".csv",sep="")
print(k)
threeGramDF <-read.csv(file = key)
threeGramDF<-threeGramDF[,-1]
for (m in 1:3) {
     name<-paste("threeGramDF",(k+m),".csv",sep="")
     temp <- read.csv(name)
     temp<-temp[,-1]
     threeGramDF <- merge(threeGramDF,temp, by.x="Tri",by.y="Tri", all=TRUE)
     print(k+m)
     rm(temp)
}
out<-paste("mergedTri",k,".csv",sep="")
counts<-threeGramDF[,2:ncol(threeGramDF)]
threeGramDF$counts<-rowSums(counts,na.rm=TRUE)
threeGramDF<-threeGramDF[,-(2:(ncol(threeGramDF)-1))]
write.csv(threeGramDF,out)
rm(threeGramDF,counts)

#####
## continue aggregating dataframes in groups to develop one trigram dataframe
#####

for (p in c(1,11,21,31,41)) {
     m1<-paste("mergedTri",p,".csv",sep="")
     m6<-paste("mergedTri",(p+5),".csv",sep="")
     print(m1)
     print(m6)
     m1<-read.csv(m1)[,-1]
     m6<-read.csv(m6)[,-1]
     super <- merge(m1,m6, by.x="Tri",by.y="Tri", all=TRUE)
     rm(m1,m6)
     counts<-super[,2:ncol(super)]
     super$counts<-rowSums(counts,na.rm=TRUE)
     super<-super[,-(2:(ncol(super)-1))]
     supOut<-paste("superTri",p,".csv",sep="")
     write.csv(super,supOut)
     rm(super,counts)
}        

superTri51<-read.csv("mergedTri51.csv")[,-1]
write.csv(superTri51,"superTri51.csv")
rm(superTri51)

#####
## Continue aggregating dataframes in groups to develop one trigram dataframe
#####

for (p in c(1,21,41)) {
     m1<-paste("superTri",p,".csv",sep="")
     m6<-paste("superTri",(p+10),".csv",sep="")
     print(m1)
     print(m6)
     m1<-read.csv(m1)[,-1]
     m6<-read.csv(m6)[,-1]
     super <- merge(m1,m6, by.x="Tri",by.y="Tri", all=TRUE)
     rm(m1,m6)
     counts<-super[,2:ncol(super)]
     super$counts<-rowSums(counts,na.rm=TRUE)
     super<-super[,-(2:(ncol(super)-1))]
     supOut<-paste("superTri2-",p,".csv",sep="")
     write.csv(super,supOut)
     rm(super,counts)
} 

#####
## First of final 2 aggregating processes to develop one trigram dataframe
#####

A<-read.csv("superTri2-1.csv")[,-1]
AtwoPlus<-A[which(A$counts>=2),]
Asingles<-A[which(A$counts==1),]
rm(A)

B<-read.csv("superTri2-21.csv")[,-1]
BtwoPlus<-B[which(B$counts>=2),]
Bsingles<-B[which(B$counts==1),]
rm(B)

# creates aggregated dataframes for singleton trigrams and all others
threegramDF <- merge(AtwoPlus,BtwoPlus, by.x="Tri",by.y="Tri", all=TRUE)
rm(AtwoPlus,BtwoPlus)
write.csv(threegramDF,"threegramDF.csv")
rm(threegramDF)
singles <-merge(Asingles,Bsingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(Asingles,Bsingles)
write.csv(singles,"interimABsingles.csv")

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri1.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
rm(singleBlend)

half<-trunc(length(singles$counts)/2)
half2<-length(singles$counts)-half

singlesFirstHalf<-singles[1:half,]
write.csv(singlesFirstHalf,"firstHalf.csv")
rm(singlesFirstHalf)
singlesSecondHalf<-singles[-(1:half),]
rm(singles)
write.csv(singlesSecondHalf,"secondHalf.csv")

#####
## Second of final 2 aggregating processes to develop one trigram dataframe
#####

C<-read.csv("superTri2-41.csv")[,-1]
CtwoPlus<-C[which(C$counts>=2),]
Csingles<-C[which(C$counts==1),]
rm(C)

threegramDF<-read.csv("threegramDF.csv")
threegramDF<-threegramDF[,-1]
threegramDF <- merge(threegramDF,CtwoPlus, by.x="Tri",by.y="Tri", all=TRUE)
rm(CtwoPlus)
counts<-threegramDF[,2:ncol(threegramDF)]
threegramDF$counts<-rowSums(counts,na.rm=TRUE)
threegramDF<-threegramDF[,-(2:(ncol(threegramDF)-1))]
rm(counts)
write.csv(threegramDF,"threegramDF.csv")
rm(threegramDF)

singles <-merge(singlesSecondHalf,Csingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(singlesSecondHalf)

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri2.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
write.csv(singles,"halfRealsingles2.csv")
rm(singleBlend,singles)

firstHalf<-read.csv("firstHalf.csv")
firstHalf<-firstHalf[,-1]
singles <-merge(firstHalf,Csingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(Csingles,firstHalf)

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri3.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
rm(singleBlend)
write.csv(singles,"halfRealSingles1.csv")
rm(singles)

####
## Bind all work together
####

threegramDF<-read.csv("threegramDF.csv")
threegramDF<-threegramDF[,-1]
one<-read.csv("add2Tri1.csv")
one<-one[-1]
threegramDF<-rbind(threegramDF,one)
rm(one)
two<-read.csv("add2Tri2.csv")
two<-two[-1]
threegramDF<-rbind(threegramDF,two)
rm(two)
three<-read.csv("add2Tri3.csv")
three<-three[-1]
threegramDF<-rbind(threegramDF,three)
rm(three)
### Resultant trigram dataframe
write.csv(threegramDF,"threegramDF.csv")

threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram)
rm(bigram)
threegramDF$Uni<-sub(".* ","",threegramDF$Bi)
threegramDF$w3<-sub(".* ","",threegramDF$Tri) # provides the suggested word
# Builds frequency of frequency table for Good-Turing smoothing
tri.freqfreq <- data.frame(Tri=table(threegramDF$counts))
write.csv(threegramDF[,-1],"threegramDF.csv")
write.csv(tri.freqfreq,"tri-freqfreq.csv")
rm(tri.freqfreq,threegramDF) 

####
## create dataframe of singleton trigrams
####
S1<-read.csv("halfRealSingles1.csv")
S1<-S1[,-1]
S2<-read.csv("halfRealSingles2.csv")
S2<-S2[,-1]
S1<-rbind(S1,S2)
write.csv(S1,"singleTriGrams.csv")
rm(S1,S2)
```

``` {r 5-generateGTmatrix, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Generates Good-Turing matrix for prediction model
##
# build empty matrix
GTCount<-matrix(c(seq(0,6,1),rep(0,21)),nrow=7,ncol=4,
                dimnames = list(c(seq(0,6,1)),c("count","uni","bi","tri")))

# read in 1-gram information
unigramDF<-read.csv("unigramDF.csv")[,-1]
unigramLength<-length(unigramDF$Uni)
unigramDF$Uni<-as.character(unigramDF$Uni)
unigramDF$counts<-as.numeric(unigramDF$counts)
uni.freqfreq<-data.frame(Uni=table(unigramDF$counts))

# Calculate probability of unseen unigram as per Jurafsky and Martin
GTCount[1,2] <- uni.freqfreq[1,2]
GTCount[2:7,2] <- uni.freqfreq[1:6,2]

kFactor <- 6*GTCount[7,2]/GTCount[2,2] # for k = 5
for (c in 0:5){
     num<-((c+1)*GTCount[c+2,2]/GTCount[c+1,2])-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,2]<-num/den
}
rm(unigramDF,uni.freqfreq)

# read in 2-gram information
twogramDF<-read.csv("twogramDF.csv")[,-1]
bigramLength<-length(twogramDF$Bi)
twogramDF$Bi<-as.character(twogramDF$Bi)
twogramDF$counts<-as.numeric(twogramDF$counts)
twogramDF$Uni<-sub(" .*","",twogramDF$Bi)

bi.freqfreq<-data.frame(Bi=table(twogramDF$counts))

# Calculate probability of unseen bigram as per Jurafsky and Martin
GTCount[1,3] <- unigramLength^2 - bigramLength
GTCount[2:7,3] <- bi.freqfreq[1:6,2]

kFactor <- 6*GTCount[7,3]/GTCount[2,3] # for k = 5
for (c in 0:5){
     num<-(c+1)*GTCount[c+2,3]/GTCount[c+1,3]-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,3]<-num/den
}
rm(twogramDF,bi.freqfreq)

# read in 3-gram information
threegramDF<-read.csv("threegramDF.csv")[,-1]
singleTri<-read.csv("singleTriGrams.csv")
singleTriLength<-length(singleTri$Tri)
trigramLength<-length(threegramDF$Tri)+singleTriLength
rm(singleTri)
threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram)
rm(bigram)
threegramDF$w3<-sub(".* ","",threegramDF$Tri)

tri.freqfreq <- data.frame(Tri=table(threegramDF$counts))
# Calculate probability of unseen trigram as per Jurafsky and Martin
GTCount[1,4] <- unigramLength^3 - trigramLength
GTCount[2,4] <- singleTriLength
GTCount[3:7,4] <- tri.freqfreq[1:5,2]

kFactor <- 6*GTCount[7,4]/GTCount[2,4] # for k = 5
for (c in 0:5){
     num<-(c+1)*GTCount[c+2,4]/GTCount[c+1,4]-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,4]<-num/den
}
write.csv(GTCount,"GTCount.csv")
rm(threegramDF,tri.freqfreq)

rm(list=ls())
```

``` {r 6-predictingngramModel, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
########
## This code relies on two .CSV datasets built using the other modules:
##    *GTCount.csv - contains the Good-Turing Smoothing parameters
##    * threegramDF.csv - contains all non-singleton trigrams and counts
########
options(digits=4)

## Read in prepared .CSV datasets
threegramDF<-read.csv("threegramDF.csv")[,-1]
threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram) # sets up the 2-gram lookup index
rm(bigram)
threegramDF$Uni<-sub(".* ","",threegramDF$Bi)
threegramDF$w3<-sub(".* ","",threegramDF$Tri) # provides list of output word

# read in Good-Turing Smoothing table
GTCount<-read.csv("GTCount.csv")[,-1]

##### Supplemental Function Calls #####
cleanInput <-function(word) {
##
## This function call takes in a user passes parameter and does basic
## preprocessing such as remove punctuation (retaining apostrophes),
## remove and preceding spaces, and remove multiple spaces in phrase.
##
## It returns the processed user input
##
    word <- tolower(word) #ensure input is in lower case
    word <- gsub("[^[:alnum:][:space:]\']", "",word)
    word <- gsub("^[ ]{1,10}","",word)
    word <- gsub("[ ]{2,10}"," ",word)
    return(word)
}

buildTable <-function(input,subTri,cluster) {
##
## This function takes in the cleaned input phrase and a subset of all
## seen tri-grams from threegramDF.csv based on user input. It also takes in
## a user defined number for the clusters they want displayed
##
## It returns the list of tri-grams for visual and tabular display
##
    diff = 0 # initialize counter - compares diff in probabilities of rows
    row.use=1 # initialize counter - counts total rows to subset from DF
    size <- dim(subTri)[1]
    if (size == 1){        # if the total trigrams found equals one
        useTri <- subTri   # simply use the one phrase and return as output
        return(useTri)
    } else {               # else build table of outputs based on cluster input
        remain = size - 1  # counter to work through list
        while (diff < cluster && remain >0) {
            if (subTri[row.use,2] - subTri[row.use+1,2] > .00001) diff=diff + 1
            row.use <-row.use + 1
            remain <- remain - 1    # calculates when end-of-list is reached
        }
        if (remain == 0) {
            useTri <- subTri[1:row.use,]  # when list is fully used
            return(useTri)
        } else {
            useTri <- subTri[1:row.use-1,]  # if max is reached first
            return(useTri)
        }
    }
}

###### Main Prediction Function #######

predict <-function(input,cluster=7){
## This function takes as input two words (a bigram) and uses that to enter
## lookup tables to find the highest probability trigrams that result.
## It emplolys Good-Turing Smoothing and Katz back off when conditions would
## suggest their use.
##
## Within the function, it produces a dotchart based on a default of 7 clusters
## The function returns a data frame contain the top predicted word endings
##
    Katz = FALSE # initialize Katz back off flag to FALSE
    gt = FALSE # initialize Good-Turing smoothing to FALSE
    input <- cleanInput(input)
    inputSize<-length(strsplit(input, " ")[[1]])
    if (inputSize != 2) stop("Please input exactly two words.\n",
        "Don't forget adding the space.")    # error handling
    nCount <- sum(threegramDF[which(threegramDF$Bi==input),2])
    if (nCount == 0) {     # bicount=0 therefore use Katz backoff
        Katz = TRUE
        input <- gsub(".* ","",input)    # isolates w2 as unigram
        nCount <- sum(threegramDF[which(threegramDF$Uni==input),2])
        if (nCount == 0) stop("This phrase is very unique.\n", 
            "I can't seem to find it.")     # error handling

        # Subset all recorded 2-grams that begin with unigram
        seekTri<-grepl(paste("^",input,"$",sep=""),threegramDF$Uni)
        subTri<-threegramDF[seekTri,] #subset relevant outputs
        # aggregation is key here because otherwise can provide
        # multiple output words as the front of bigrams was removed
        subTri<-aggregate(subTri$counts,list(subTri$w3),sum)
        names(subTri)<-c("w3","counts")
        subTri<-subTri[order(subTri$counts,decreasing=T),]
        useTri <- buildTable (input,subTri,cluster)
        for (i in 1:length(useTri$counts)) {
            count = useTri[i,2]
            if(count<=5) {     # employs the Good-Turing Smoothing
                useTri[i,2]<-GTCount[count+1,2]
                gt = TRUE
            }
        }
    } else {

        # Subset all recorded 3-grams that begin with bigram
        seekTri<-grepl(paste("^",input,"$",sep=""),threegramDF$Bi)
        subTri<-threegramDF[seekTri,] #subset relevant 3-grams
        subTri<-aggregate(subTri$counts,list(subTri$w3),sum)
        names(subTri)<-c("w3","counts")
        subTri<-subTri[order(subTri$counts,decreasing=T),]
        useTri <- buildTable (input,subTri,cluster)
        for (i in 1:length(useTri$counts)) {
            count = useTri[i,2]
            if(count<=5) {     # employs the Good-Turing Smoothing
                useTri[i,2]<-GTCount[count+1,4]
                gt = TRUE
            }
        }
    }
   
    options(digits = 4)

    ## generates data frame of tabular outputs for user review
    predictWord <- data.frame(Word=useTri$w3,
        probability=(useTri$counts/nCount)*100, stringsAsFactors=FALSE) 
    
    ## generates dotchart to visualize possible options, must invert order
    plot<-predictWord[order(predictWord$probability),] #order for lowest to highest
    dotchart(plot$probability,labels=plot$Word, pch=19, color="blue",
         xlab=paste("Probability (in %) of top",cluster,"clusters"),
         main=paste("N-Grams Starting with: \"",toupper(input),"\""))
    
    ## informative phrases as to what the user input was
    print(paste("Words completing N-Gram starting with: ",toupper(input)))
    if(Katz==TRUE){
        print("*Katz back off helped find options by reducing 2-gram to a 1-gram")
    }    
    if(gt == TRUE){
        print("*Good-Turing techniques were used in the solution")
    }
    
    return(predictWord)
}
```


# Acknowledgements
First, I would like to very much like to thank my wife - my parter - for all her understanding and support. Not only during this Capstone course, but through the entire sequence of the Data Science Specialization. She is *amazing* and for that I am eternally grateful. Thanks also to Dr. Thomas Bock for his explanation of key some mathematical concepts. Lastly, I would be remiss if I did not acknowledge the power of collaborative group dynamics. Many, many ideas and solutions are rooted in part to the generous comments from my colleagues on the Capstone Discussion Forum. I would like to thank and acknowledge the entire beta class who provide sage advice and support in that collaborative space.

# References

Feinerer, I. Hornik, K., & Artifex Software. (2014, June 11). Tm: Text mining package (Version 0.6) [Software]. Available from [http://cran.r-project.org/web/packages/tm/index.html]( http://cran.r-project.org/web/packages/tm/index.html).

Feinerer, I., Hornik, K., & Meyer, D. (2008, March). Text mining infrastructure in R. *Journal of Statistical Computing*, 25(5). Retrieved from [http://www.jstatsoft.org/v25/i05/paper](http://www.jstatsoft.org/v25/i05/paper).

Jurafsky, D. & Martin, J.H. (2000). *Speech and language processing: An introduction to natural language processing, computational linguistics and speech recognition*. Englewood Cliffs, NJ: Prentice Hall.

R Core Team (2014, July 10). R: A language and environment for statistical computing. R Foundation for Statistical Computing (Version 3.1.1) [Software]. Vienna, Austria. Available from [http://www.R-project.org/]( http://www.R-project.org/).

Richards, B. (1987). Type/token ratios:What do they really tell us? *Journal of Child Language*, 14, pp. 201209. Doi: 10:1017/S0305000900012885.

Turing, A.M. (1950). Computing machinery and intelligence. *Mind*, 59, 433-460. Retrieved from [http://www.loebner.net/Prizef/TuringArticle.html](http://www.loebner.net/Prizef/TuringArticle.html).


******
# Appendices

## Appendix A - Generating Training and Testing Sets

``` {r A-createDataSubsets, echo=TRUE, eval=FALSE, warning=FALSE,error=FALSE,cache=TRUE}
##
## Create data subsets from original datasets provided
##

###
## Note: prior to running, create these three folders and extract the associated data into each - one corpus per folder:
# Folder Names: blogs, news, twitter
###

library(tm) # Use tm package
# Read in three English text data sets already extracted from .zip file
# Used virtual corpus for this interim work
tweet <- VCorpus(DirSource("twitter", encoding = "UTF-8"),
                 readerControl=list(language="en"))
blog <- VCorpus(DirSource("blogs", encoding = "UTF-8"),
                readerControl=list(language="en"))
news <- VCorpus(DirSource("news", encoding = "UTF-8"),
                readerControl=list(language="en"))
########
## Approach to split corpora into train, devtest, and test sets
########

# Permute entire tweet group to randomize order
set.seed(0330)
perm.tweet <- sample(tweet[[2]][[1]], length(tweet[[2]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.tweet))
twtTrain <- perm.tweet[1:TR]
remain<-perm.tweet[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
twtDevtest <- remain[1:DEV]
twtTest <- remain[-(1:DEV)]
write(twtTrain,"twtTrain.txt")
write(twtDevtest,"twtDevtest.txt")
write(twtTest,"twtTest.txt")
rm(list = ls()) #clean environment

# Permute entire blog group to randomize order                     
set.seed(0330)
perm.blog <- sample(blog[[1]][[1]], length(blog[[1]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.blog))
blogTrain <- perm.blog[1:TR]
remain<-perm.blog[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
blogDevtest <- remain[1:DEV]
blogTest <- remain[-(1:DEV)]
write(blogTrain,"blogTrain.txt")
write(blogDevtest,"blogDevtest.txt")
write(blogTest,"blogTest.txt")
rm(list = ls()) #clean environment

# Permute entire news group to randomize order   
set.seed(0330)
perm.news <- sample(news[[1]][[1]], length(news[[1]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.news))
newsTrain <- perm.news[1:TR]
remain<-perm.news[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
newsDevtest <- remain[1:DEV]
newsTest <- remain[-(1:DEV)]
write(newsTrain,"newsTrain.txt")
write(newsDevtest,"newsDevtest.txt")
write(newsTest,"newsTest.txt")
rm(list = ls()) #clean environment
```

******
## Appendix B - Reading in the Data and Cleaning

``` {r B-readTraininSettoClean, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
#
## Read in dataset and clean for use
##

###
## Note: after creating the blog training set "blogTrain.txt from the previous code, add a folder "training" and include "blogTrain.txt" in it.
## Also and create folder "mod" to hold in-process cleaning data
###

library(tm)   # package used to read dataset
library(filehash) # package used to update/maintain permanent corpus database
myCorpus <- PCorpus(DirSource("training", #training contains blog 60% training set
     encoding="UTF-8",mode="text"),dbControl=list(dbName="myCorpus.db", dbType="DB1"))
##
## Cleaning steps refined  over time
## Accomplished in three loops due to RAM limitations
##

# to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower));dbInit("myCorpus.db")

for (j in seq(myCorpus)) {
     # first two separate hyphenated and slashed words
     myCorpus[[j]][[1]] <-gsub("-", " ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("/", " ", myCorpus[[j]][[1]])
     # converts symbol <> into an apostrophe
     myCorpus[[j]][[1]] <-gsub("<>", "\\'", myCorpus[[j]][[1]])
     print("3 of 18 transformations complete") #provides progress to user
     # these three create end of sentence markers <EOS>
     myCorpus[[j]][[1]] <-gsub("\\. |\\.$","  <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\? |\\?$","  <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\! |\\!$","  <EOS> ", myCorpus[[j]][[1]])
     print("6 of 18 transformations complete") #provides progress to user
}
write(myCorpus[[1]][[1]],"./mod/blogTrain.txt") # writes corpus to permanent disc
# reads back in corpus...this method reduced 908MB corpus to 137MB
myCorpus <- PCorpus(DirSource("mod", 
     encoding="UTF-8",mode="text"),dbControl=list(dbName="halfCorpus.db", dbType="DB1"))

for (j in seq(myCorpus)) {
     # tranforms various ASCII codes to appropriate language 
     myCorpus[[j]][[1]] <-gsub("<85>"," <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("<92>","'", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\&", " and ", myCorpus[[j]][[1]])
     print("9 of 18 transformations complete") #provides progress to user
     # removes all punctuation except apostrophe and <> symbols in <EOS>
     myCorpus[[j]][[1]] <-gsub("[^[:alnum:][:space:]\'<>]", " ", myCorpus[[j]][[1]])
     # removes web site URLs
     myCorpus[[j]][[1]] <-gsub(" www(.+) ", " ", myCorpus[[j]][[1]])
     # removes all single letters except "a" and "i"
     myCorpus[[j]][[1]] <-gsub(" [b-hj-z] "," ", myCorpus[[j]][[1]])
     print("12 of 18 transformations complete") #provides progress to user
}

write(myCorpus[[1]][[1]],"./mod/blogTrain.txt")  # writes corpus to permanent disc
# reduced 937 MB corpus to 133 MB
myCorpus <- PCorpus(DirSource("mod",
     encoding="UTF-8",mode="text"),dbControl=list(dbName="lastCorpus.db", dbType="DB1"))

for (j in seq(myCorpus)) {
     # removes errant apostrohes introduced by transformations
     myCorpus[[j]][[1]] <-gsub(" ' "," ", myCorpus[[j]][[1]])        
     myCorpus[[j]][[1]] <-gsub("\\' ", " ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub(" ' ", " ", myCorpus[[j]][[1]])
     print("15 of 18 transformations complete") #provides progress to user
     # removes errant codes in < > brackets
     myCorpus[[j]][[1]] <-gsub("<[^EOS].+>"," ", myCorpus[[j]][[1]])
     # replaces numbers with a number marker <NUM> for context
     myCorpus[[j]][[1]] <-gsub("[0-9]+"," <NUM> ", myCorpus[[j]][[1]])
     # removes and errant <> brackets remaining
     myCorpus[[j]][[1]] <-gsub("<>"," ", myCorpus[[j]][[1]])
     print("18 of 18 transformations complete") #provides progress to user
}
# removes numbers and the tm package "dbInit" compresses data in RAM
myCorpus <- tm_map(myCorpus, removeNumbers);dbInit("lastCorpus.db")
# removes errant 's symbols not as contractions
myCorpus[[1]][[1]] <-gsub(" 's"," ", myCorpus[[1]][[1]])
# removes errant close brackets starting a word
myCorpus[[1]][[1]] <-gsub(">[a-z]"," ", myCorpus[[1]][[1]])

myCorpus <- tm_map(myCorpus, stripWhitespace);dbInit("lastCorpus.db")
# final corpus after all processing only 111 MB

# Writes final, processed corpus to disc for building n-grams
write(myCorpus[[1]][[1]],"./mod/blogTrain.txt") # write PCorpus to disc
# original training set 130 MB; clean training seet 108 MB
```

******
## Appendix C - Prototype Modeling - Building N-Gram Models

### C-1: Building the 1-Gram (unigram) Model

``` {r C1-buildOneGramModel, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Code uses blogTrain.txt to generate list of all 1-gram (unigrams)
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="aggCorpus.db", dbType="DB1"))

library(tau) #package to create N-Grams
ngram <- function(n) {
     textcnt(CORP, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]]) #pulls out the text element from the list myCorpus

oneGram <- ngram(1)
unigramDF<-data.frame(Uni = names(oneGram), counts = unclass(oneGram))
rm(oneGram)
unigramDF$Uni<-as.character(unigramDF$Uni)
unigramDF$counts<-as.numeric(unigramDF$counts)
# removes the "words" <eos> and <num> from unigram table
unigramDF <- unigramDF[which(unigramDF$Uni!="<eos>"),]
unigramDF <- unigramDF[which(unigramDF$Uni!="<num>"),]

lengthUni<-length(unigramDF$Uni) #253,921 unigrams

# Builds frequency of frequency table for Good-Turing smoothing
uni.freqfreq<-data.frame(Uni=table(unigramDF$counts))

write.csv(unigramDF,"unigramDF.csv") #2,620 frequencies
write.csv(uni.freqfreq,"uni.freqfreq.csv")
rm(unigramDF,uni.freqfreq,CORP,myCorpus)
```


### C-2: Building the 2-Gram (bigram) Model
``` {r C2-buildTwoGramModel, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Code uses blogTrain.txt to generate list of all 2-grams (bigrams)
## Specifically designed to break large corpus into chunks to deal
## with limitations imposed by RAM in R
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="biCorpus.db", dbType="DB1"))
library(tau) #package to create N-Grams
ngram <- function(n) {
     textcnt(CORPport, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]]) #pulls out the text element from the list myCorpus
rm(myCorpus) #reduce RAM load
# determine number of loop runs to process 10,000 docs per run
step<-trunc(length(CORP)/10000)
remain<- length(CORP)-(step * 10000)
# first deal with the remainder
CORPport<-CORP[1:remain]
# invokes tau package to build the bigrams
twoGram <- ngram(2)
names(twoGram) <- gsub("^\'","",names(twoGram))        
twogramDF<-data.frame(Bi = names(twoGram), counts = unclass(twoGram))
names(twogramDF)<-c("Bi","counts")
## removes the "words" <eos> and <num> from unigram table
eosTag<-grepl("<eos>",twogramDF$Bi)
twogramDF<-twogramDF[!eosTag,]
numTag<-grepl("<num>",twogramDF$Bi)
twogramDF<-twogramDF[!numTag,]
# writes first of N:n=step dataframes
write.csv (twogramDF,"twoGramDF1.csv")
CORP<-CORP[-(1:remain)] #remove already processed docs from corpus

for (i in 1:(step-1)) {  # loop to process steps of 10000 docs
     CORPport<-CORP[1:10000]
     twoGram <- ngram(2)
     names(twoGram) <- gsub("^\'","",names(twoGram))        
     tmptwogramDF<-data.frame(Bi = names(twoGram), counts = unclass(twoGram))
     print(paste("Iteration",i,"of",step)) #provide user progress
     name<-paste("twoGramDF",(i+1),".csv",sep="")
     ## removes the "words" <eos> and <num> from unigram table
     eosTag<-grepl("<eos>",tmptwogramDF$Bi)
     tmptwogramDF<-tmptwogramDF[!eosTag,]
     numTag<-grepl("<num>",tmptwogramDF$Bi)
     tmptwogramDF<-tmptwogramDF[!numTag,]
     write.csv (tmptwogramDF,name)
     twogramDF<-rbind(twogramDF,tmptwogramDF)
     twogramDF<-aggregate(twogramDF$counts,list(Bi=twogramDF$Bi),sum)
     names(twogramDF)<-c("Bi","counts")
     CORP<-CORP[-(1:10000)]
}
rm(tmptwogramDF,numTag,eosTag,ngram,CORPport,twoGram)
rm(remain,step,name,i,CORP)
twogramDF$Bi<-as.character(twogramDF$Bi)
twogramDF$counts<-as.numeric(twogramDF$counts)
twogramDF$Uni<-sub(" .*","",twogramDF$Bi)
# Builds frequency of frequency table for Good-Turing smoothing
bi.freqfreq<-data.frame(Bi=table(twogramDF$counts))
write.csv(twogramDF,"twogramDF.csv")
write.csv(bi.freqfreq,"bi-freqfreq.csv")
rm(bi.freqfreq,twogramDF)
```

### C-3: Building the 3-Gram (trigram) Model
``` {r C3-buildThreeGramModel, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Code uses blogTrain.txt to generate list of all 3-grams (trigrams)
## Specifically designed to break large corpus into chunks to deal
## with limitations imposed by RAM in R
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="triCorpus.db", dbType="DB1"))
library(tau)
ngram <- function(n) {
     textcnt(CORPport, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]])
rm(myCorpus) #reduce RAM load
# determine number of loop runs to process 10,000 docs per run
step<-trunc(length(CORP)/10000)
for (i in 1:(step)) {
     CORPport<-CORP[1:10000]
     # invokes tau package to build the trigrams
     threeGram <- ngram(3)
     names(threeGram) <- gsub("^\'","",names(threeGram))
     threegramDF<-data.frame(Tri = names(threeGram), counts = unclass(threeGram))
     print(paste("Iteration",i,sep=" "))
     name<-paste("threeGramDF",i,".csv",sep="")
     ## removes the "words" <eos> and <num> from trigram table
     eosTag<-grepl("<eos>",threegramDF$Tri)
     threegramDF<-threegramDF[!eosTag,]
     numTag<-grepl("<num>",threegramDF$Tri)
     threegramDF<-threegramDF[!numTag,]
     # writes first of N:n=step dataframes
     write.csv (threegramDF,name)
     CORP<-CORP[-(1:10000)]
}
CORPport<-CORP
# repeats above loop for the remaining documents (partial step)
threeGram <- ngram(3)
names(threeGram) <- gsub("^\'","",names(threeGram))
threegramDF<-data.frame(Tri = names(threeGram), counts = unclass(threeGram))
names(threegramDF)<-c("Tri","counts")
name<-paste("threeGramDF",i+1,".csv",sep="")
eosTag<-grepl("<eos>",threegramDF$Tri)
threegramDF<-threegramDF[!eosTag,]
numTag<-grepl("<num>",threegramDF$Tri)
threegramDF<-threegramDF[!numTag,]
write.csv (threegramDF,name)

rm(threegramDF, CORP,CORPport,threeGram,i,name)

#####
## Begin aggregating dataframes in groups to develop one trigram dataframe
#####

for (k in seq(1,46,5)) {
     key<-paste("threeGramDF",k,".csv",sep="")
     print(k)
     threeGramDF <-read.csv(file = key)
     threeGramDF<-threeGramDF[,-1]
     for (m in 1:4) {
          name<-paste("threeGramDF",(k+m),".csv",sep="")
          temp <- read.csv(name)
          temp<-temp[,-1]
          threeGramDF <- merge(threeGramDF,temp, by.x="Tri",by.y="Tri", all=TRUE)
          print(k+m)
          rm(temp)
     }
     out<-paste("mergedTri",k,".csv",sep="")
     counts<-threeGramDF[,2:ncol(threeGramDF)]
     threeGramDF$counts<-rowSums(counts,na.rm=TRUE)
     threeGramDF<-threeGramDF[,-(2:(ncol(threeGramDF)-1))]
     write.csv(threeGramDF,out)
     rm(threeGramDF,counts)
}

k=k+5

key<-paste("threeGramDF",k,".csv",sep="")
print(k)
threeGramDF <-read.csv(file = key)
threeGramDF<-threeGramDF[,-1]
for (m in 1:3) {
     name<-paste("threeGramDF",(k+m),".csv",sep="")
     temp <- read.csv(name)
     temp<-temp[,-1]
     threeGramDF <- merge(threeGramDF,temp, by.x="Tri",by.y="Tri", all=TRUE)
     print(k+m)
     rm(temp)
}
out<-paste("mergedTri",k,".csv",sep="")
counts<-threeGramDF[,2:ncol(threeGramDF)]
threeGramDF$counts<-rowSums(counts,na.rm=TRUE)
threeGramDF<-threeGramDF[,-(2:(ncol(threeGramDF)-1))]
write.csv(threeGramDF,out)
rm(threeGramDF,counts)

#####
## continue aggregating dataframes in groups to develop one trigram dataframe
#####

for (p in c(1,11,21,31,41)) {
     m1<-paste("mergedTri",p,".csv",sep="")
     m6<-paste("mergedTri",(p+5),".csv",sep="")
     print(m1)
     print(m6)
     m1<-read.csv(m1)[,-1]
     m6<-read.csv(m6)[,-1]
     super <- merge(m1,m6, by.x="Tri",by.y="Tri", all=TRUE)
     rm(m1,m6)
     counts<-super[,2:ncol(super)]
     super$counts<-rowSums(counts,na.rm=TRUE)
     super<-super[,-(2:(ncol(super)-1))]
     supOut<-paste("superTri",p,".csv",sep="")
     write.csv(super,supOut)
     rm(super,counts)
}        

superTri51<-read.csv("mergedTri51.csv")[,-1]
write.csv(superTri51,"superTri51.csv")
rm(superTri51)

#####
## Continue aggregating dataframes in groups to develop one trigram dataframe
#####

for (p in c(1,21,41)) {
     m1<-paste("superTri",p,".csv",sep="")
     m6<-paste("superTri",(p+10),".csv",sep="")
     print(m1)
     print(m6)
     m1<-read.csv(m1)[,-1]
     m6<-read.csv(m6)[,-1]
     super <- merge(m1,m6, by.x="Tri",by.y="Tri", all=TRUE)
     rm(m1,m6)
     counts<-super[,2:ncol(super)]
     super$counts<-rowSums(counts,na.rm=TRUE)
     super<-super[,-(2:(ncol(super)-1))]
     supOut<-paste("superTri2-",p,".csv",sep="")
     write.csv(super,supOut)
     rm(super,counts)
} 

#####
## First of final 2 aggregating processes to develop one trigram dataframe
#####

A<-read.csv("superTri2-1.csv")[,-1]
AtwoPlus<-A[which(A$counts>=2),]
Asingles<-A[which(A$counts==1),]
rm(A)

B<-read.csv("superTri2-21.csv")[,-1]
BtwoPlus<-B[which(B$counts>=2),]
Bsingles<-B[which(B$counts==1),]
rm(B)

# creates aggregated dataframes for singleton trigrams and all others
threegramDF <- merge(AtwoPlus,BtwoPlus, by.x="Tri",by.y="Tri", all=TRUE)
rm(AtwoPlus,BtwoPlus)
write.csv(threegramDF,"threegramDF.csv")
rm(threegramDF)
singles <-merge(Asingles,Bsingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(Asingles,Bsingles)
write.csv(singles,"interimABsingles.csv")

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri1.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
rm(singleBlend)

half<-trunc(length(singles$counts)/2)
half2<-length(singles$counts)-half

singlesFirstHalf<-singles[1:half,]
write.csv(singlesFirstHalf,"firstHalf.csv")
rm(singlesFirstHalf)
singlesSecondHalf<-singles[-(1:half),]
rm(singles)
write.csv(singlesSecondHalf,"secondHalf.csv")

#####
## Second of final 2 aggregating processes to develop one trigram dataframe
#####

C<-read.csv("superTri2-41.csv")[,-1]
CtwoPlus<-C[which(C$counts>=2),]
Csingles<-C[which(C$counts==1),]
rm(C)

threegramDF<-read.csv("threegramDF.csv")
threegramDF<-threegramDF[,-1]
threegramDF <- merge(threegramDF,CtwoPlus, by.x="Tri",by.y="Tri", all=TRUE)
rm(CtwoPlus)
counts<-threegramDF[,2:ncol(threegramDF)]
threegramDF$counts<-rowSums(counts,na.rm=TRUE)
threegramDF<-threegramDF[,-(2:(ncol(threegramDF)-1))]
rm(counts)
write.csv(threegramDF,"threegramDF.csv")
rm(threegramDF)

singles <-merge(singlesSecondHalf,Csingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(singlesSecondHalf)

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri2.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
write.csv(singles,"halfRealsingles2.csv")
rm(singleBlend,singles)

firstHalf<-read.csv("firstHalf.csv")
firstHalf<-firstHalf[,-1]
singles <-merge(firstHalf,Csingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(Csingles,firstHalf)

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri3.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
rm(singleBlend)
write.csv(singles,"halfRealSingles1.csv")
rm(singles)

####
## Bind all work together
####

threegramDF<-read.csv("threegramDF.csv")
threegramDF<-threegramDF[,-1]
one<-read.csv("add2Tri1.csv")
one<-one[-1]
threegramDF<-rbind(threegramDF,one)
rm(one)
two<-read.csv("add2Tri2.csv")
two<-two[-1]
threegramDF<-rbind(threegramDF,two)
rm(two)
three<-read.csv("add2Tri3.csv")
three<-three[-1]
threegramDF<-rbind(threegramDF,three)
rm(three)
### Resultant trigram dataframe
write.csv(threegramDF,"threegramDF.csv")

threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram)
rm(bigram)
threegramDF$Uni<-sub(".* ","",threegramDF$Bi)
threegramDF$w3<-sub(".* ","",threegramDF$Tri) # provides the suggested word
# Builds frequency of frequency table for Good-Turing smoothing
tri.freqfreq <- data.frame(Tri=table(threegramDF$counts))
write.csv(threegramDF[,-1],"threegramDF.csv")
write.csv(tri.freqfreq,"tri-freqfreq.csv")
rm(tri.freqfreq,threegramDF) 

####
## create dataframe of singleton trigrams
####
S1<-read.csv("halfRealSingles1.csv")
S1<-S1[,-1]
S2<-read.csv("halfRealSingles2.csv")
S2<-S2[,-1]
S1<-rbind(S1,S2)
write.csv(S1,"singleTriGrams.csv")
rm(S1,S2)
```

******
## Appendix D - Implementing Good-Turing Smoothing

``` {r D-generateGTmatrix, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Generates Good-Turing matrix for prediction model
##
# build empty matrix
GTCount<-matrix(c(seq(0,6,1),rep(0,21)),nrow=7,ncol=4,
                dimnames = list(c(seq(0,6,1)),c("count","uni","bi","tri")))

# read in 1-gram information
unigramDF<-read.csv("unigramDF.csv")[,-1]
unigramLength<-length(unigramDF$Uni)
unigramDF$Uni<-as.character(unigramDF$Uni)
unigramDF$counts<-as.numeric(unigramDF$counts)
uni.freqfreq<-data.frame(Uni=table(unigramDF$counts))

# Calculate probability of unseen unigram as per Jurafsky and Martin
GTCount[1,2] <- uni.freqfreq[1,2]
GTCount[2:7,2] <- uni.freqfreq[1:6,2]

kFactor <- 6*GTCount[7,2]/GTCount[2,2] # for k = 5
for (c in 0:5){
     num<-((c+1)*GTCount[c+2,2]/GTCount[c+1,2])-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,2]<-num/den
}
rm(unigramDF,uni.freqfreq)

# read in 2-gram information
twogramDF<-read.csv("twogramDF.csv")[,-1]
bigramLength<-length(twogramDF$Bi)
twogramDF$Bi<-as.character(twogramDF$Bi)
twogramDF$counts<-as.numeric(twogramDF$counts)
twogramDF$Uni<-sub(" .*","",twogramDF$Bi)

bi.freqfreq<-data.frame(Bi=table(twogramDF$counts))

# Calculate probability of unseen bigram as per Jurafsky and Martin
GTCount[1,3] <- unigramLength^2 - bigramLength
GTCount[2:7,3] <- bi.freqfreq[1:6,2]

kFactor <- 6*GTCount[7,3]/GTCount[2,3] # for k = 5
for (c in 0:5){
     num<-(c+1)*GTCount[c+2,3]/GTCount[c+1,3]-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,3]<-num/den
}
rm(twogramDF,bi.freqfreq)

# read in 3-gram information
threegramDF<-read.csv("threegramDF.csv")[,-1]
singleTri<-read.csv("singleTriGrams.csv")
singleTriLength<-length(singleTri$Tri)
trigramLength<-length(threegramDF$Tri)+singleTriLength
rm(singleTri)
threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram)
rm(bigram)
threegramDF$w3<-sub(".* ","",threegramDF$Tri)

tri.freqfreq <- data.frame(Tri=table(threegramDF$counts))
# Calculate probability of unseen trigram as per Jurafsky and Martin
GTCount[1,4] <- unigramLength^3 - trigramLength
GTCount[2,4] <- singleTriLength
GTCount[3:7,4] <- tri.freqfreq[1:5,2]

kFactor <- 6*GTCount[7,4]/GTCount[2,4] # for k = 5
for (c in 0:5){
     num<-(c+1)*GTCount[c+2,4]/GTCount[c+1,4]-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,4]<-num/den
}
write.csv(GTCount,"GTCount.csv")
rm(threegramDF,tri.freqfreq)

rm(list=ls())
```

******
## Appendix E - Prediction Model

``` {r E-predictingngramModel, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
########
## This code relies on two .CSV datasets built using the other modules:
##    *GTCount.csv - contains the Good-Turing Smoothing parameters
##    * threegramDF.csv - contains all non-singleton trigrams and counts
########
options(digits=4)

## Read in prepared .CSV datasets
threegramDF<-read.csv("threegramDF.csv")[,-1]
threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram) # sets up the 2-gram lookup index
rm(bigram)
threegramDF$Uni<-sub(".* ","",threegramDF$Bi)
threegramDF$w3<-sub(".* ","",threegramDF$Tri) # provides list of output word

# read in Good-Turing Smoothing table
GTCount<-read.csv("GTCount.csv")[,-1]

##### Supplemental Function Calls #####
cleanInput <-function(word) {
##
## This function call takes in a user passes parameter and does basic
## preprocessing such as remove punctuation (retaining apostrophes),
## remove and preceding spaces, and remove multiple spaces in phrase.
##
## It returns the processed user input
##
    word <- tolower(word) #ensure input is in lower case
    word <- gsub("[^[:alnum:][:space:]\']", "",word)
    word <- gsub("^[ ]{1,10}","",word)
    word <- gsub("[ ]{2,10}"," ",word)
    return(word)
}

buildTable <-function(input,subTri,cluster) {
##
## This function takes in the cleaned input phrase and a subset of all
## seen tri-grams from threegramDF.csv based on user input. It also takes in
## a user defined number for the clusters they want displayed
##
## It returns the list of tri-grams for visual and tabular display
##
    diff = 0 # initialize counter - compares diff in probabilities of rows
    row.use=1 # initialize counter - counts total rows to subset from DF
    size <- dim(subTri)[1]
    if (size == 1){        # if the total trigrams found equals one
        useTri <- subTri   # simply use the one phrase and return as output
        return(useTri)
    } else {               # else build table of outputs based on cluster input
        remain = size - 1  # counter to work through list
        while (diff < cluster && remain >0) {
            if (subTri[row.use,2] - subTri[row.use+1,2] > .00001) diff=diff + 1
            row.use <-row.use + 1
            remain <- remain - 1    # calculates when end-of-list is reached
        }
        if (remain == 0) {
            useTri <- subTri[1:row.use,]  # when list is fully used
            return(useTri)
        } else {
            useTri <- subTri[1:row.use-1,]  # if max is reached first
            return(useTri)
        }
    }
}

###### Main Prediction Function #######

predict <-function(input,cluster=7){
## This function takes as input two words (a bigram) and uses that to enter
## lookup tables to find the highest probability trigrams that result.
## It emplolys Good-Turing Smoothing and Katz back off when conditions would
## suggest their use.
##
## Within the function, it produces a dotchart based on a default of 7 clusters
## The function returns a data frame contain the top predicted word endings
##
    Katz = FALSE # initialize Katz back off flag to FALSE
    gt = FALSE # initialize Good-Turing smoothing to FALSE
    input <- cleanInput(input)
    inputSize<-length(strsplit(input, " ")[[1]])
    if (inputSize != 2) stop("Please input exactly two words.\n",
        "Don't forget adding the space.")    # error handling
    nCount <- sum(threegramDF[which(threegramDF$Bi==input),2])
    if (nCount == 0) {     # bicount=0 therefore use Katz backoff
        Katz = TRUE
        input <- gsub(".* ","",input)    # isolates w2 as unigram
        nCount <- sum(threegramDF[which(threegramDF$Uni==input),2])
        if (nCount == 0) stop("This phrase is very unique.\n", 
            "I can't seem to find it.")     # error handling

        # Subset all recorded 2-grams that begin with unigram
        seekTri<-grepl(paste("^",input,"$",sep=""),threegramDF$Uni)
        subTri<-threegramDF[seekTri,] #subset relevant outputs
        # aggregation is key here because otherwise can provide
        # multiple output words as the front of bigrams was removed
        subTri<-aggregate(subTri$counts,list(subTri$w3),sum)
        names(subTri)<-c("w3","counts")
        subTri<-subTri[order(subTri$counts,decreasing=T),]
        useTri <- buildTable (input,subTri,cluster)
        for (i in 1:length(useTri$counts)) {
            count = useTri[i,2]
            if(count<=5) {     # employs the Good-Turing Smoothing
                useTri[i,2]<-GTCount[count+1,2]
                gt = TRUE
            }
        }
    } else {

        # Subset all recorded 3-grams that begin with bigram
        seekTri<-grepl(paste("^",input,"$",sep=""),threegramDF$Bi)
        subTri<-threegramDF[seekTri,] #subset relevant 3-grams
        subTri<-aggregate(subTri$counts,list(subTri$w3),sum)
        names(subTri)<-c("w3","counts")
        subTri<-subTri[order(subTri$counts,decreasing=T),]
        useTri <- buildTable (input,subTri,cluster)
        for (i in 1:length(useTri$counts)) {
            count = useTri[i,2]
            if(count<=5) {     # employs the Good-Turing Smoothing
                useTri[i,2]<-GTCount[count+1,4]
                gt = TRUE
            }
        }
    }
   
    options(digits = 4)

    ## generates data frame of tabular outputs for user review
    predictWord <- data.frame(Word=useTri$w3,
        probability=(useTri$counts/nCount)*100, stringsAsFactors=FALSE) 
    
    ## generates dotchart to visualize possible options, must invert order
    plot<-predictWord[order(predictWord$probability),] #order for lowest to highest
    dotchart(plot$probability,labels=plot$Word, pch=19, color="blue",
         xlab=paste("Probability (in %) of top",cluster,"clusters"),
         main=paste("N-Grams Starting with: \"",toupper(input),"\""))
    
    ## informative phrases as to what the user input was
    print(paste("Words completing N-Gram starting with: ",toupper(input)))
    if(Katz==TRUE){
        print("*Katz back off helped find options by reducing 2-gram to a 1-gram")
    }    
    if(gt == TRUE){
        print("*Good-Turing techniques were used in the solution")
    }
    
    return(predictWord)
}
```

******
## Appendix F - Glossary of Key Terms

**Good-Turing Discounting:** Termed Good-Turing smoothing, it is a technique to re-estimate probability mass to assign N-Grams with zero or low counts by discounting from those occurring more often.")

**Katz back off:** Termed back off N-Gram modeling, it was developed in 1987 by Katz which predicts first based on non-zero, higher-order N-Grams and will "backoff" to a lower-order N-Gram if there is zero evidence of the higher-order N-Gram.

**N-Gram:** A special type of wordform which looks *(N - n) words into the past and possesses the memoryless properties of a Markov model. A 3-gram looks at the previous 2-gram to make the prediction.

**N-Gram cluster:** In calculating N-Grams, many probabilities are actually counts based on Good-Turing Discounting. A cluster is a group of words (sometimes a large number) having the same likelihood of appearing in the prediction model.

******
## Appendix G - Development Platform Information

``` {r H-sysInfo, echo=TRUE}
sessionInfo()
```
