---
title: 'Natural Language Processing: A Model to Predict a Sequence of Words'
author: 'Gerald Gendron - [LinkedIn: jaygendron](http://www.linkedin.com/in/jaygendron/)'
date: "October 13, 2014"
output:
  word_document: default
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

# Executive Summary

This report provides documentation describing the process and decisions used to develop a predictive text model for the Data Science Capstone project. All code used in the development of the project is contained in this report. The main body of the report provides the essential discussion of the product development story by summarizing key aspects of the analysis and model building. Key learning points are included to aid readers interested in reproducing this work and enhancing it. Overall, the discipline of Natural Language Processing is a broad and useful domain of data science. This report includes a brief literature review capturing key concepts that guided this project. The corpus used in this analysis might be considered to have a personality – specifically, that it is a unique collection of words, phrases, and sentences that has been characterized by some exploratory analysis to become familiar with what the corpus is and how it may be used for prediction. The initial model created from a small subset of the corpus was useful but did not enable scalability. Additional research, creative thinking, and persistent modeling alterations resulted in a predictive text model that balanced accuracy with scalability. The model described in this report was ultimately hosted as a web-based application for consumer use and allows some customization of results by users. 

*Keywords: natural language processing, text mining, predictive text analytics, N-Gram, Good-Turing Smoothing, Katz back off*

``` {r 0-createDataSubsets, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}

##################################
##################################
## For RMD purposes, please note that the entire body of code is place within 
## the document to allow for runs of graphics. You will notice that most front
## "Chunks" are set to echo=FALSE and eval=FALSE. This is done to enable 
## generation of the PDF report
##################################
##################################


##
## Create data subsets from original datasets provided
##

###
## Note: prior to running, create these three folders and extract the associated
## data into each - one corpus per folder:
# Folder Names: blogs, news, twitter
###

library(tm) # Use tm package
# Read in three English text data sets already extracted from .zip file
# Used virtual corpus for this interim work
tweet <- VCorpus(DirSource("twitter", encoding = "UTF-8"), 
                 readerControl=list(language="en"))
blog <- VCorpus(DirSource("blogs", encoding = "UTF-8"),
                readerControl=list(language="en"))
news <- VCorpus(DirSource("news", encoding = "UTF-8"),
                readerControl=list(language="en"))
########
## Approach to split corpora into train, devtest, and test sets
########

# Permute entire tweet group to randomize order
set.seed(0330)
perm.tweet <- sample(tweet[[2]][[1]], length(tweet[[2]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.tweet))
twtTrain <- perm.tweet[1:TR]
remain<-perm.tweet[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
twtDevtest <- remain[1:DEV]
twtTest <- remain[-(1:DEV)]
write(twtTrain,"twtTrain.txt")
write(twtDevtest,"twtDevtest.txt")
write(twtTest,"twtTest.txt")
rm(list = ls()) #clean environment

# Permute entire blog group to randomize order                     
set.seed(0330)
perm.blog <- sample(blog[[1]][[1]], length(blog[[1]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.blog))
blogTrain <- perm.blog[1:TR]
remain<-perm.blog[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
blogDevtest <- remain[1:DEV]
blogTest <- remain[-(1:DEV)]
write(blogTrain,"blogTrain.txt")
write(blogDevtest,"blogDevtest.txt")
write(blogTest,"blogTest.txt")
rm(list = ls()) #clean environment

# Permute entire news group to randomize order   
set.seed(0330)
perm.news <- sample(news[[1]][[1]], length(news[[1]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.news))
newsTrain <- perm.news[1:TR]
remain<-perm.news[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
newsDevtest <- remain[1:DEV]
newsTest <- remain[-(1:DEV)]
write(newsTrain,"newsTrain.txt")
write(newsDevtest,"newsDevtest.txt")
write(newsTest,"newsTest.txt")
rm(list = ls()) #clean environment
```

# Understanding the Problem

A most important aspect at the outset of a data analysis project is to understand the problem. With the advent of social media and blogs, the information value of raw text continues to increase. The problem that exists is in analyzing a large corpus of text to discover the structure and arrangement of words within the data in order to analyze the corpus using computational methods. 
The essence of this project is to take a corpus (a body) of text from various sources, clean and analyze that text data, and build a predictive model to present the next likely word in a stream of text provided by a user. User input could range from formal, professional communication to informal, short messages - such as social media. Therefore, knowledge of the data sources in the corpus is essential. As a concrete example, a user may type into their mobile device - "I would like to". A predictive text model would present the most likely options for what the next word might be such as *"eat"*, *"go"*, or *"have"* - to name a few.

Data sciences are increasingly making use of Natural Language Processing combined with statistical methods developed within the arts and humanities decades ago to characterize and leverage the streams of data that are text based and not inherently quantitative. There are many techniques available within the R programming language to work with text to work with them quantitatively. A key aspect of this project is to discern which techniques best promote accuracy and scalability for large data sets.

# Literature Review

**An origin of natural language processing**

Alan Turing (1950) opens his influential article "Computing Machinery and Intelligence" with the statement, "I propose to consider the question, 'Can machines think?'" (p. 433). He follows this by outlining something he calls the imagination game played by man A - known as label X, woman B - known as label Y, and interrogator C. The interrogator is able to ask questions of X or Y in order to attain the objective to properly identify whether "X is A and Y is B" (p. 433) or vice versa. Turing later refines the original question to read, "Are there imaginable digital computers which would do well in the imitation game?" (p. 442). In essence - to predict truth based on features or interrogation. He speaks to a machine teaching process made up of rewards and punishments enforcing orders in symbolic language. Overall, this Turing Test has become a basis of natural language processing - covering a broad array of uses such as spelling correction, speech recognition, author identification, and prediction of words based on preceding words. 

**Literature review purpose and findings**

At the outset of this project, course instructors provided us with various sources on natural language processing, text mining, and various R programming packages they felt would be useful. Further literature review was conducted in order to understand the field and extract clues for building a model. Primary goals of the literature review were to understand:

* common issues when analyzing text data
* the body of knowledge that has built up in the domain of natural language processing
* other resources not provided by this course for helping us in the analysis methodology

This section is not a comprehensive overview of over 40 sources reviewed but merely a summary of two works most influential in shaping the modeling approach taken in this project. 

Course notes for the Capstone project invited participants to become familiar with the R packages on NLP `openNLP` and text mining `tm`. Feinerer, Hornik, and Meyer (2008) provide a good overview of the essential aspects of these packages. Noteworthy was their information about: reading in corpora into the R environment, explaining functions to transform the data, explaining stemming, stopwords, and tagging parts of speech, considering the issue of text sparsity, and understanding the fundamental of count based analysis. 

Jurafsky and Martin (2000) provide a seminal work within the domain of NLP. The authors present a key approach for building prediction models called the N-Gram, which relies on knowledge of word sequences from *(N – 1)* prior words. It is a type of language model based on counting words in the corpora to establish probabilities about next words. Overall, Jurafsky and Martin's work had the greatest influence on this project in choosing among many possible strategies for developing a model to predict word selection. It addresses multiple perspectives of the topics found in Feinerer, Hornik, and Meyer. The following approaches and assumptions were chosen For purposes of building an initial model. They may be adapted as models are refined to increase prediction accuracy. 

1. Case: corpora words will not be case-sensitive. Although important for spelling correction and part of speech analysis, the words themselves - not their case - are important for prediction. 
2. Stopwords: similarly, unlike classification and clustering applications, all words will be included in the model as they represent more than just the primary carriers of the message. 
3. Wordform: stemming will not be used as N-Grams are typically based on wordforms (unique, inflected forms of words). So whereas *table* and *tables* are the same lemma, they will be treated as separate words in this model. 
4. Punctuation: Jurafsky and Martin treat punctuation as a word and counted as a word. Given the nature of the SwiftKey approach, which is not trying to generate full sentences but only predict a next word, punctuation will be treated slightly differently in the initial model. End of sentence punctuation (like ? ' ! .) will be used to include end-of-sentence <EOS> tags as the intuition is they have implications for word prediction. 
5. Parts of Speech: the discussion of N-Grams did not imply the inherent value of predication based on syntactically using parts of speech.
6. Numbers: there is no intuition based on the research that numbers will have a great impact on a predication model and they will be removed 
7. Sparse Words: all words will be retained. A key concept from Jurafsky and Martin is the idea that even bigram models are quite sparse; however, rather than eliminating those wordforms, they become clues to the "probability of unseen N-Grams" (p. 209, 2000). They include as their fourth of eight key concepts *Things Seen Once* and recommend using the count of wordforms seen a single time to provide a basis to estimate those things not seen in the training set and will likely appear in a test set 
8. Whitespace: this was not discussed directly by Jurafsky and Martin. The intuition is that whitespace has little to do with context and excess whitespace will be removed

In addition to shaping the initial strategy, the work by Jurafsky and Martin provided valuable insights on other aspects of implementing a predication model.

* Historical context of the unsmoothed N-Gram and basis of probabilistic approach to predicting words using the Markov assumption to simplify probabilities by looking only at (n-1) or (n-2) previous words 
* Importance of having a diverse corpora in order to improve generalizability of prediction among other corpora 
* Comparative analysis of smoothing and discounting techniques to increase predictive power 
*Introduction of back off techniques to establish probabilities for otherwise unseen elements of an N-Gram 
* Discussion of entropy and perplexity - which may prove to be a good choice as the single measure to help assess quality of the prediction model

# Data Processing

We have to understand the data, determine what should be done with the data, and generate the questions that need to be asked to ascertain whether the data is sufficient to do the job.This section briefly addresses the acquisition, processing, and exploration of the data.

**Data Acquisition and Cleaning**

SwiftKey is the corporate partner involved with this project. They produce software to aid users in rapidly entering text with higher accuracy on mobile devices. Based on their collaboration with Johns Hopkins University, a [data set](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) was provided to the data scientists on this project. The dataset is a zip files including blog posts, news articles, and Twitter tweets in four languages (English, German, Finnish, and Russian). The data was downloaded using the R programming language (R Core Team, 2014) and the elements were extracted using the R text-mining package called `tm` (Feinerer. Hornik, & Artifex Software, 2014). The `PCorpus` function was used as it establishes a permanent database instead of a virtual one. This allowed a database to hold over 3.3 million documents in physical disk memory rather than completely in RAM to reserve processing capacity. <a href="#appendix-a---generating-training-and-testing-sets">Appendix A - Generating 
Training and Testing Sets</a> provides the code and `seeds` used to generate the training and testing datasets.

``` {r 1a-readTraininSettoClean, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Read in dataset and clean for use. The DirSource "training" is a pointer to 
## the location of the training corpus created
##
library(tm)   # package used to read dataset
library(filehash) # package used to update/maintain permanent corpus database
myCorpus <- PCorpus(DirSource("training", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="myCorpus.db", dbType="DB1"))
```

A preliminary exploration of the corpus indicated a number of transformations were required on the raw data to prepare it for statistical modeling. The data set was cleaned using over 20 transformations that pre-processed the data for analysis. <a href="#appendix-b---reading-in-the-data-and-cleaning">Appendix B - Reading in the Data and Cleaning</a> fully presents each of the transformations in detail. In general, those transformations included: conversion to lower case, ensuring apostrophes were retained to maintain contractions, remove numbers, and remove excess whitespace. Intermittently in the process, the corpus was written back to disk and the database was re-initialized using the `filehash` package to reduce the size of data processing in RAM. Although one third of the way through processing the corpus had exceeded 1 GB in RAM, use of `filehash` kept this well below that. The final corpus of over 539,573 blog documents after all processing only 108 MB. It is worth noting the original data set of 899,288 blogs was over 205 MB.

``` {r 1b-readTraininSettoClean, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Cleaning steps refined  over time
## Accomplished in three loops due to RAM limitations
##

# to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower));dbInit("myCorpus.db")

for (j in seq(myCorpus)) {
     # first two separate hyphenated and slashed words
     myCorpus[[j]][[1]] <-gsub("-", " ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("/", " ", myCorpus[[j]][[1]])
     # converts symbol <> into an apostrophe
     myCorpus[[j]][[1]] <-gsub("<>", "\\'", myCorpus[[j]][[1]])
     print("3 of 18 transformations complete") #provides progress to user
     # these three create end of sentence markers <EOS>
     myCorpus[[j]][[1]] <-gsub("\\. |\\.$","  <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\? |\\?$","  <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\! |\\!$","  <EOS> ", myCorpus[[j]][[1]])
     print("6 of 18 transformations complete") #provides progress to user
}
write(myCorpus[[1]][[1]],"./mod/blogTrain.txt") # writes corpus to permanent disc
# reads back in corpus...this method reduced 908MB corpus to 137MB
myCorpus <- PCorpus(DirSource("mod", 
     encoding="UTF-8",mode="text"),
          dbControl=list(dbName="halfCorpus.db", dbType="DB1"))

for (j in seq(myCorpus)) {
     # tranforms various ASCII codes to appropriate language 
     myCorpus[[j]][[1]] <-gsub("<85>"," <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("<92>","'", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\&", " and ", myCorpus[[j]][[1]])
     print("9 of 18 transformations complete") #provides progress to user
     # removes all punctuation except apostrophe and <> symbols in <EOS>
     myCorpus[[j]][[1]] <-gsub("[^[:alnum:][:space:]\'<>]", " ",
          myCorpus[[j]][[1]])
     # removes web site URLs
     myCorpus[[j]][[1]] <-gsub(" www(.+) ", " ", myCorpus[[j]][[1]])
     # removes all single letters except "a" and "i"
     myCorpus[[j]][[1]] <-gsub(" [b-hj-z] "," ", myCorpus[[j]][[1]])
     print("12 of 18 transformations complete") #provides progress to user
}

write(myCorpus[[1]][[1]],"./mod/blogTrain.txt")  # writes corpus to disc
# reduced 937 MB corpus to 133 MB
myCorpus <- PCorpus(DirSource("mod",
     encoding="UTF-8",mode="text"),dbControl=list(dbName="lastCorpus.db",
          dbType="DB1"))

for (j in seq(myCorpus)) {
     # removes errant apostrohes introduced by transformations
     myCorpus[[j]][[1]] <-gsub(" ' "," ", myCorpus[[j]][[1]])        
     myCorpus[[j]][[1]] <-gsub("\\' ", " ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub(" ' ", " ", myCorpus[[j]][[1]])
     print("15 of 18 transformations complete") #provides progress to user
     # removes errant codes in < > brackets
     myCorpus[[j]][[1]] <-gsub("<[^EOS].+>"," ", myCorpus[[j]][[1]])
     # replaces numbers with a number marker <NUM> for context
     myCorpus[[j]][[1]] <-gsub("[0-9]+"," <NUM> ", myCorpus[[j]][[1]])
     # removes and errant <> brackets remaining
     myCorpus[[j]][[1]] <-gsub("<>"," ", myCorpus[[j]][[1]])
     print("18 of 18 transformations complete") #provides progress to user
}
# removes numbers and the tm package "dbInit" compresses data in RAM
myCorpus <- tm_map(myCorpus, removeNumbers);dbInit("lastCorpus.db")
# removes errant 's symbols not as contractions
myCorpus[[1]][[1]] <-gsub(" 's"," ", myCorpus[[1]][[1]])
# removes errant close brackets starting a word
myCorpus[[1]][[1]] <-gsub(">[a-z]"," ", myCorpus[[1]][[1]])

myCorpus <- tm_map(myCorpus, stripWhitespace);dbInit("lastCorpus.db")
# final corpus after all processing only 111 MB

# Writes final, processed corpus to disc for building n-grams
write(myCorpus[[1]][[1]],"./mod/blogTrain.txt") # write PCorpus to disc
# original training set 130 MB; clean training seet 108 MB
```

**Exploratory Analysis**

Clean data is a necessary – but not sufficient – condition to developing a prediction algorithm. This step is about understanding: understanding the relationships between the words and sentences, and other observable artifacts useful to set expectations in the model development. Highlights of the many hours of exploration including an understanding of relationships between vocabulary size and unique words, distributions of various N-Grams, and information that helped reevaluate the original strategy after the literature review. Table 1 provides statistics on the full corpora. It shows the total number of documents in the each genre of the corpus. These values also represent the total line counts because the loading method treated each article as a single line regardless of length. For instance, the longest line is a blog of 40,833 characters. 

*Table 1: Characterizing the Corpora by Word Count, Type, Ratios, Diversity*
       
Source | Documents | Vocabulary (V) | Word Types (T) | TTR (T/V) | Diversity 
------ | --------- | -------------- | -------------- | --------- | ---------
Blog   |   899,288 |     37,334,131 |      1,103,548 |   0.030   |  127.71
News   |    77,259 |      2,643,969 |        197,858 |   0.075   |   86.04
Tweets | 2,360,148 |     30,373,543 |      1,290,170 |   0.042   |  165.53
Corpus | 3,336,695 |     70,351,643 |      2,123,809 |   0.030   |  179.04

Table 1 also shows the total vocabulary (V) equaling the number of total word tokens present in each genre. Just over half the complete Corpus is composed of the blog posts. Word types (T) are the number of unique words within the Vocabulary. The Type/Token Ratio(TTR) is a well known measure of language comparison. It is simply the total word types divided by vocabulary (Richards, 1987).  The TTR indicates complexity, with higher numbers indicating a more complex genre. Tweets are the most complex because it takes more unique words to build a smaller vocabulary. This measure was used to make a decision to limit the data for the model to just blog entries. News articles tend to be overly repetitive with non-conversational language and tweets are a language unto themselves with many "created" words.

Diversity is also provided in Table 1. According to Richards (1987) it is a more useful measure because TTR will tend to fall off as a function of growing vocabulary alone. Diversity is defined as "A measure of vocabulary diversity that is approximately independent of sample size is the number of different words divided by the square root of twice the number of words in the sample" (Richards, p.208). It is robust and is positively correlated to the tokens and it flattens out at some point. Table 2 shows the effect on diversity as the size of the vocabulary (document numbers) increases. There is a relative flattening out at 60 percent of the total documents. This is in line with a common technique for separating data into a 60 percent training set, a 20 percent test set, and a 20 percent validation set. This helped validate the selection of using a training set composed of 60 percent of all blogs in the dataset. Notice in Table 3 that the widely used measure of Type/Token Ratio shows the similarity in complexity as represented by the blogs and the entire corpora of blogs, news, and tweets.

*Table 2: Effect of vocabulary size on diversity measures*

 Measure  |  Type  |   50  |   500 | 5,000 |   50K  |   60%  |   80%  | Entire
--------- | ------ | ----- | ----- | ----- | ------ | ------ | ------ | ------
Diversity |  Blog  | 16.38 | 34.17 | 57.31 |  83.07 | 118.55 | 123.72 | 127.71
Diversity | Corpus | 22.68 | 45.22 | 72.56 | 103.84 | 163.43 | 172.10 | 179.05

*Table 3: Effect of vocabulary size on Type/Token Ratios*

 Measure  |  Type  |   50  |   500 | 5,000 |   50K  |   60%  |   80%  | Entire
--------- | ------ | ----- | ----- | ----- | ------ | ------ | ------ | ------
TTR       |  Blog  |  0.51 |  0.31 |  0.18 |   0.08 |   0.04 |   0.03 |   0.03
TTR       | Corpus |  0.49 |  0.29 |  0.15 |   0.07 |   0.04 |   0.03 |   0.03

Understanding the distribution among the word tokens helps shape expectations of the linguistic model. An N-Gram refers to the number of words in a string. This project will work on a 3-Gram model. The basic building blocks of that model are unigrams (N: n=1), bigrams (N: n=2), and trigrams (N: n=3). The code developed to build each of these three N-Gram models is available in these appendices:

* <a href="#c-1-building-the-1-gram-unigram-model">C-1: Building the 1-Gram (unigram) Model</a>
* <a href="#c-2-building-the-2-gram-bigram-model">C-2: Building the 2-Gram (bigram) Model</a> 
* <a href="#c-3-building-the-3-gram-trigram-model">C-3: Building the 3-Gram (trigram) Model</a>

``` {r 2-buildOneGramModel, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE,message=FALSE}
##
## Code uses blogTrain.txt to generate list of all 1-gram (unigrams)
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="aggCorpus.db", dbType="DB1"))

library(tau) #package to create N-Grams
ngram <- function(n) {
     textcnt(CORP, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]]) #pulls out the text element from the list myCorpus

oneGram <- ngram(1)
unigramDF<-data.frame(Uni = names(oneGram), counts = unclass(oneGram))
rm(oneGram)
unigramDF$Uni<-as.character(unigramDF$Uni)
unigramDF$counts<-as.numeric(unigramDF$counts)
# removes the "words" <eos> and <num> from unigram table
unigramDF <- unigramDF[which(unigramDF$Uni!="<eos>"),]
unigramDF <- unigramDF[which(unigramDF$Uni!="<num>"),]

lengthUni<-length(unigramDF$Uni) #253,921 unigrams

# Builds frequency of frequency table for Good-Turing smoothing
uni.freqfreq<-data.frame(Uni=table(unigramDF$counts))

write.csv(unigramDF,"unigramDF.csv") #2,620 frequencies
write.csv(uni.freqfreq,"uni.freqfreq.csv")
rm(unigramDF,uni.freqfreq,CORP,myCorpus)
```

``` {r 3-buildTwoGramModel, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Code uses blogTrain.txt to generate list of all 2-grams (bigrams)
## Specifically designed to break large corpus into chunks to deal
## with limitations imposed by RAM in R
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="biCorpus.db", dbType="DB1"))
library(tau) #package to create N-Grams
ngram <- function(n) {
     textcnt(CORPport, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]]) #pulls out the text element from the list myCorpus
rm(myCorpus) #reduce RAM load
# determine number of loop runs to process 10,000 docs per run
step<-trunc(length(CORP)/10000)
remain<- length(CORP)-(step * 10000)
# first deal with the remainder
CORPport<-CORP[1:remain]
# invokes tau package to build the bigrams
twoGram <- ngram(2)
names(twoGram) <- gsub("^\'","",names(twoGram))        
twogramDF<-data.frame(Bi = names(twoGram), counts = unclass(twoGram))
names(twogramDF)<-c("Bi","counts")
## removes the "words" <eos> and <num> from unigram table
eosTag<-grepl("<eos>",twogramDF$Bi)
twogramDF<-twogramDF[!eosTag,]
numTag<-grepl("<num>",twogramDF$Bi)
twogramDF<-twogramDF[!numTag,]
# writes first of N:n=step dataframes
write.csv (twogramDF,"twoGramDF1.csv")
CORP<-CORP[-(1:remain)] #remove already processed docs from corpus

for (i in 1:(step-1)) {  # loop to process steps of 10000 docs
     CORPport<-CORP[1:10000]
     twoGram <- ngram(2)
     names(twoGram) <- gsub("^\'","",names(twoGram))        
     tmptwogramDF<-data.frame(Bi = names(twoGram), counts = unclass(twoGram))
     print(paste("Iteration",i,"of",step)) #provide user progress
     name<-paste("twoGramDF",(i+1),".csv",sep="")
     ## removes the "words" <eos> and <num> from unigram table
     eosTag<-grepl("<eos>",tmptwogramDF$Bi)
     tmptwogramDF<-tmptwogramDF[!eosTag,]
     numTag<-grepl("<num>",tmptwogramDF$Bi)
     tmptwogramDF<-tmptwogramDF[!numTag,]
     write.csv (tmptwogramDF,name)
     twogramDF<-rbind(twogramDF,tmptwogramDF)
     twogramDF<-aggregate(twogramDF$counts,list(Bi=twogramDF$Bi),sum)
     names(twogramDF)<-c("Bi","counts")
     CORP<-CORP[-(1:10000)]
}
rm(tmptwogramDF,numTag,eosTag,ngram,CORPport,twoGram)
rm(remain,step,name,i,CORP)
twogramDF$Bi<-as.character(twogramDF$Bi)
twogramDF$counts<-as.numeric(twogramDF$counts)
twogramDF$Uni<-sub(" .*","",twogramDF$Bi)
# Builds frequency of frequency table for Good-Turing smoothing
bi.freqfreq<-data.frame(Bi=table(twogramDF$counts))
write.csv(twogramDF,"twogramDF.csv")
write.csv(bi.freqfreq,"bi-freqfreq.csv")
rm(bi.freqfreq,twogramDF)
```

``` {r 4-buildThreeGramModel, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Code uses blogTrain.txt to generate list of all 3-grams (trigrams)
## Specifically designed to break large corpus into chunks to deal
## with limitations imposed by RAM in R
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="triCorpus.db", dbType="DB1"))
library(tau)
ngram <- function(n) {
     textcnt(CORPport, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]])
rm(myCorpus) #reduce RAM load
# determine number of loop runs to process 10,000 docs per run
step<-trunc(length(CORP)/10000)
for (i in 1:(step)) {
     CORPport<-CORP[1:10000]
     # invokes tau package to build the trigrams
     threeGram <- ngram(3)
     names(threeGram) <- gsub("^\'","",names(threeGram))
     threegramDF<-data.frame(Tri = names(threeGram), counts = unclass(threeGram))
     print(paste("Iteration",i,sep=" "))
     name<-paste("threeGramDF",i,".csv",sep="")
     ## removes the "words" <eos> and <num> from trigram table
     eosTag<-grepl("<eos>",threegramDF$Tri)
     threegramDF<-threegramDF[!eosTag,]
     numTag<-grepl("<num>",threegramDF$Tri)
     threegramDF<-threegramDF[!numTag,]
     # writes first of N:n=step dataframes
     write.csv (threegramDF,name)
     CORP<-CORP[-(1:10000)]
}
CORPport<-CORP
# repeats above loop for the remaining documents (partial step)
threeGram <- ngram(3)
names(threeGram) <- gsub("^\'","",names(threeGram))
threegramDF<-data.frame(Tri = names(threeGram), counts = unclass(threeGram))
names(threegramDF)<-c("Tri","counts")
name<-paste("threeGramDF",i+1,".csv",sep="")
eosTag<-grepl("<eos>",threegramDF$Tri)
threegramDF<-threegramDF[!eosTag,]
numTag<-grepl("<num>",threegramDF$Tri)
threegramDF<-threegramDF[!numTag,]
write.csv (threegramDF,name)

rm(threegramDF, CORP,CORPport,threeGram,i,name)

#####
## Begin aggregating dataframes in groups to develop one trigram dataframe
#####

for (k in seq(1,46,5)) {
     key<-paste("threeGramDF",k,".csv",sep="")
     print(k)
     threeGramDF <-read.csv(file = key)
     threeGramDF<-threeGramDF[,-1]
     for (m in 1:4) {
          name<-paste("threeGramDF",(k+m),".csv",sep="")
          temp <- read.csv(name)
          temp<-temp[,-1]
          threeGramDF <- merge(threeGramDF,temp, by.x="Tri",by.y="Tri", all=TRUE)
          print(k+m)
          rm(temp)
     }
     out<-paste("mergedTri",k,".csv",sep="")
     counts<-threeGramDF[,2:ncol(threeGramDF)]
     threeGramDF$counts<-rowSums(counts,na.rm=TRUE)
     threeGramDF<-threeGramDF[,-(2:(ncol(threeGramDF)-1))]
     write.csv(threeGramDF,out)
     rm(threeGramDF,counts)
}

k=k+5

key<-paste("threeGramDF",k,".csv",sep="")
print(k)
threeGramDF <-read.csv(file = key)
threeGramDF<-threeGramDF[,-1]
for (m in 1:3) {
     name<-paste("threeGramDF",(k+m),".csv",sep="")
     temp <- read.csv(name)
     temp<-temp[,-1]
     threeGramDF <- merge(threeGramDF,temp, by.x="Tri",by.y="Tri", all=TRUE)
     print(k+m)
     rm(temp)
}
out<-paste("mergedTri",k,".csv",sep="")
counts<-threeGramDF[,2:ncol(threeGramDF)]
threeGramDF$counts<-rowSums(counts,na.rm=TRUE)
threeGramDF<-threeGramDF[,-(2:(ncol(threeGramDF)-1))]
write.csv(threeGramDF,out)
rm(threeGramDF,counts)

#####
## continue aggregating dataframes in groups to develop one trigram dataframe
#####

for (p in c(1,11,21,31,41)) {
     m1<-paste("mergedTri",p,".csv",sep="")
     m6<-paste("mergedTri",(p+5),".csv",sep="")
     print(m1)
     print(m6)
     m1<-read.csv(m1)[,-1]
     m6<-read.csv(m6)[,-1]
     super <- merge(m1,m6, by.x="Tri",by.y="Tri", all=TRUE)
     rm(m1,m6)
     counts<-super[,2:ncol(super)]
     super$counts<-rowSums(counts,na.rm=TRUE)
     super<-super[,-(2:(ncol(super)-1))]
     supOut<-paste("superTri",p,".csv",sep="")
     write.csv(super,supOut)
     rm(super,counts)
}        

superTri51<-read.csv("mergedTri51.csv")[,-1]
write.csv(superTri51,"superTri51.csv")
rm(superTri51)

#####
## Continue aggregating dataframes in groups to develop one trigram dataframe
#####

for (p in c(1,21,41)) {
     m1<-paste("superTri",p,".csv",sep="")
     m6<-paste("superTri",(p+10),".csv",sep="")
     print(m1)
     print(m6)
     m1<-read.csv(m1)[,-1]
     m6<-read.csv(m6)[,-1]
     super <- merge(m1,m6, by.x="Tri",by.y="Tri", all=TRUE)
     rm(m1,m6)
     counts<-super[,2:ncol(super)]
     super$counts<-rowSums(counts,na.rm=TRUE)
     super<-super[,-(2:(ncol(super)-1))]
     supOut<-paste("superTri2-",p,".csv",sep="")
     write.csv(super,supOut)
     rm(super,counts)
} 

#####
## First of final 2 aggregating processes to develop one trigram dataframe
#####

A<-read.csv("superTri2-1.csv")[,-1]
AtwoPlus<-A[which(A$counts>=2),]
Asingles<-A[which(A$counts==1),]
rm(A)

B<-read.csv("superTri2-21.csv")[,-1]
BtwoPlus<-B[which(B$counts>=2),]
Bsingles<-B[which(B$counts==1),]
rm(B)

# creates aggregated dataframes for singleton trigrams and all others
threegramDF <- merge(AtwoPlus,BtwoPlus, by.x="Tri",by.y="Tri", all=TRUE)
rm(AtwoPlus,BtwoPlus)
write.csv(threegramDF,"threegramDF.csv")
rm(threegramDF)
singles <-merge(Asingles,Bsingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(Asingles,Bsingles)
write.csv(singles,"interimABsingles.csv")

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri1.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
rm(singleBlend)

half<-trunc(length(singles$counts)/2)
half2<-length(singles$counts)-half

singlesFirstHalf<-singles[1:half,]
write.csv(singlesFirstHalf,"firstHalf.csv")
rm(singlesFirstHalf)
singlesSecondHalf<-singles[-(1:half),]
rm(singles)
write.csv(singlesSecondHalf,"secondHalf.csv")

#####
## Second of final 2 aggregating processes to develop one trigram dataframe
#####

C<-read.csv("superTri2-41.csv")[,-1]
CtwoPlus<-C[which(C$counts>=2),]
Csingles<-C[which(C$counts==1),]
rm(C)

threegramDF<-read.csv("threegramDF.csv")
threegramDF<-threegramDF[,-1]
threegramDF <- merge(threegramDF,CtwoPlus, by.x="Tri",by.y="Tri", all=TRUE)
rm(CtwoPlus)
counts<-threegramDF[,2:ncol(threegramDF)]
threegramDF$counts<-rowSums(counts,na.rm=TRUE)
threegramDF<-threegramDF[,-(2:(ncol(threegramDF)-1))]
rm(counts)
write.csv(threegramDF,"threegramDF.csv")
rm(threegramDF)

singles <-merge(singlesSecondHalf,Csingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(singlesSecondHalf)

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri2.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
write.csv(singles,"halfRealsingles2.csv")
rm(singleBlend,singles)

firstHalf<-read.csv("firstHalf.csv")
firstHalf<-firstHalf[,-1]
singles <-merge(firstHalf,Csingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(Csingles,firstHalf)

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri3.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
rm(singleBlend)
write.csv(singles,"halfRealSingles1.csv")
rm(singles)

####
## Bind all work together
####

threegramDF<-read.csv("threegramDF.csv")
threegramDF<-threegramDF[,-1]
one<-read.csv("add2Tri1.csv")
one<-one[-1]
threegramDF<-rbind(threegramDF,one)
rm(one)
two<-read.csv("add2Tri2.csv")
two<-two[-1]
threegramDF<-rbind(threegramDF,two)
rm(two)
three<-read.csv("add2Tri3.csv")
three<-three[-1]
threegramDF<-rbind(threegramDF,three)
rm(three)
### Resultant trigram dataframe
write.csv(threegramDF,"threegramDF.csv")

threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram)
rm(bigram)
threegramDF$Uni<-sub(".* ","",threegramDF$Bi)
threegramDF$w3<-sub(".* ","",threegramDF$Tri) # provides the suggested word
# Builds frequency of frequency table for Good-Turing smoothing
tri.freqfreq <- data.frame(Tri=table(threegramDF$counts))
write.csv(threegramDF[,-1],"threegramDF.csv")
write.csv(tri.freqfreq,"tri-freqfreq.csv")
rm(tri.freqfreq,threegramDF) 

####
## create dataframe of singleton trigrams
####
S1<-read.csv("halfRealSingles1.csv")
S1<-S1[,-1]
S2<-read.csv("halfRealSingles2.csv")
S2<-S2[,-1]
S1<-rbind(S1,S2)
write.csv(S1,"singleTriGrams.csv")
rm(S1,S2)
```

A glimpse at the distribution of the frequencies of each of these word forms is provided in Figure 1. 

*Figure 1: Distributions of three N-Gram models*

``` {r preparePlotData, echo=FALSE, eval=FALSE, cache=TRUE}
##
## Read in necessary outputs describing the N-Grams
# read in 1-gram information
unigramDF<-read.csv("unigramDF.csv")[,-1]
unigramDF$Uni<-as.character(unigramDF$Uni)
unigramDF$counts<-as.numeric(unigramDF$counts)
# read in 2-gram information
twogramDF<-read.csv("twogramDF.csv")[,-1]
twogramDF<-twogramDF[,-3]
twogramDF<-twogramDF[order(twogramDF$counts,decreasing=T),]
twogramDF$counts<-as.numeric(twogramDF$counts)
# read in 3-gram information
threegramDF<-read.csv("threegramDF.csv")[,-1]
threegramDF<-threegramDF[order(threegramDF$counts,decreasing=T),]
threegramDF$counts<-as.numeric(threegramDF$counts)
```

```{r generateDistibution plots, eval=TRUE, echo=FALSE, fig.height=2.5, cache=TRUE}
## Plot distributions of N-Grams
par(mfrow=c(1,3))
distUni<-round(0.5*dim(unigramDF)[[1]])
distBi<-round(0.5*dim(twogramDF)[[1]])
distTri<-round(0.5*17021221 )

plot(log10(unigramDF$counts[1:distUni]),ylab="log10 (Frequency)",
        xlab="Top 50% of 1-Grams", col="darkslateblue",ylim=c(.00001,6))
plot(log10(twogramDF$counts[1:distBi]),ylab="log10 (Frequency)",
        xlab="Top 50% of 2-Grams", col="darkslateblue",ylim=c(.00001,6))
plot(log10(threegramDF$counts[1:distTri]),ylab="log10 (Frequency)",
        xlab="Top 50% of 3-Grams", col="darkslateblue",ylim=c(.00001,6))
```

Prior to plotting, the frequencies were sorted in decreasing order and the x-axis is an index. Therefore, the far left of each plot indicates an N-Gram with the highest frequency. Notice a few features of the data. Although there are far fewer unigrams (198,576) than bigrams (3,525,643), than trigrams (17,021,221), they distributions are more there are a much more skewed right as the level of N increases. This indicates very long tails (more frequencies of 1, 2, etc.) but at the same time much higher frequencies of particular words (such as "the", "and", "to", etc.). The three plots are all logarithmic in the y-axis and set at a common y-axis limit to aid in comparison. In logarithmic scale log10(1)=0. Figure 1 gives an idea of how many singletons exist:
* within the first half of all unigrams, no frequencies of "1" are seen
* for bigrams, single frequencies occur at about 30 percent (around 750,00)
* for trigrams, single frequencies occur after only 10 percent of 3-grams 
appear (of note, all single 3-grams were removed from the dataframe due to RAM 
- therefore, no plot points appear at log10(frequency)=0)

Jurafsky and Martin (2000) also describe the importance of "frequencies of frequencies". This is the frequency of the frequency 20 (let's say) occurring. Those tables were generated based on. As a concrete example, assume the 1-gram "cat" appeared three times in the corpus. Also, assume that the words "dog", "flower", and "automobile" appeared three times in the corpus. The frequency of frequency for k equal to three is the value four. In other words, the frequency "three" appeared four times. Hence, the term frequency of frequencies. Hence the term "frequency of frequencies". In other words, we examine Nc, the number of N-grams that occur c times. We refer to the number of N-grams that occur c times as the frequency of frequency c" (Jurafsky & Martin, p. 212). A visualization of these frequencies is provided in Figure 2. 

*Figure 2: Viewing frequencies of frequencies of various N-Grams*

``` {r freqFreq, echo=FALSE, eval=TRUE, fig.height=2.5, cache=TRUE}
par(mfrow = c(1,3))
uni.freqfreq<-read.csv("uni.freqfreq.csv")
bi.freqfreq<-read.csv("bi-freqfreq.csv")
tri.freqfreq<-read.csv("tri-freqfreq.csv")
#hist(log10(uni.freqfreq$Uni.Freq),breaks=28)
library(scales)     
scatter.smooth(log10(uni.freqfreq$Uni.Var), log10(uni.freqfreq$Uni.Freq),
               ylab="log10 (freqency of freqency)",xlab="log10 (unigram count)",
               col=alpha("black",0.1),ylim=c(.000001,7),xlim=c(.000001,6))
scatter.smooth(log10(bi.freqfreq$Bi.Var), log10(bi.freqfreq$Bi.Freq),
               ylab="log10 (freqency of freqency)", xlab="log10 (bigram count)",
               col=alpha("black",0.1),ylim=c(.000001,7),xlim=c(.000001,6))
scatter.smooth(log10(tri.freqfreq$Tri.Var), log10(tri.freqfreq$Tri.Freq),
               ylab="log10(freqency of freqency)", xlab="log10 (trigram count)",
               col=alpha("black",0.1),ylim=c(.000001,7),xlim=c(.000001,6))
```

The frequencies of frequencies view of the corpus is an important feature for N-Grams because these counts enable predictions of other words. The frequencies reduce rapidly comparing unigrams to trigrams. Information from the e literature indicate that these frequency tables respond well to regression analysis to smooth out the information and avoid having to keep all the data in storage. A visualization of these frequencies is provided in Figure 2.

# Statistical Modeling

The overall method used two generate the first model is based on the techniques we learned in the specialization course. Specifically to take the full corpus and break it up into a training set, a developmental test set, and a test set (terms from Jurafsky and Martin). The objective of the modeling phase is to create a model that has that balances accuracy with speed and scalability given that we expect a very large data set and the working corpus. Then we can can revise algorithms to improve either accuracy, speed, or both if possible.

**Predictive modeling**

As noted in the literature review, the strategy selected was to build an N-Gram model augmented with Good Turing Smoothing methods. Initially, it appeared a full Markov matrix would be necessary; however, it was the Markovian properties afforded by N-Gram modeling. In other words, it does not matter the length of the phrase – one can predict a word on a trigram, bigram, or even unigram (Jurafsky and Martin, 2000).

Continuing with the cleaned and explored corpus, the `tau` package in the R programming environment was used to tokenize and build N-Grams at the N:n=1, 2, and 3 levels. This proved to be a very efficient way to store the data because it collapsed entire matrices into small number of columns (between three and five), albeit having very long numbers of observations. To give an idea of the size of the N-Grams the table below shows the number of observations for the 1-, 2-, 3-Gram models.

N-Gram |  Instances
------ | -----------
1-Gram |    198,576
2-Gram |  3,525,643
3-Gram | 17,021,221

Having these N-Gram models in dataframes, a Good-Turing matrix was generated. As noted in Jurafsky and Martin (2000), any N-gram with a frequency less than (K:k=5) is an ideal candidate to recomputed counts using Good-Turing methods. N-Grams appearing six or more times are seen to be realistic and do not require smoothing. The power of the Good-Turing method is that it allows us to calculate a probability of the unseen N-Gram. In order to do this it must discount the probabilities of the seen N-Grams in order for the probabilities to add up to one. As mentioned above, discounting in this predictive model was applied to all frequencies of frequencies between the numbers one in five. Those discounted probabilities go towards the probability that the N-Gram appears zero times. Details on the code used in the Good Turing algorithm is available in <a href="#appendix-d---implementing-good-turing-smoothing">Appendix D - Implementing Good-Turing Smoothing</a>. The Good-Turing Smoothing matrix for this model is presented in Table 4.
 
``` {r 5-generateGTmatrix, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Generates Good-Turing matrix for prediction model
##
# build empty matrix
GTCount<-matrix(c(seq(0,6,1),rep(0,21)),nrow=7,ncol=4,
                dimnames = list(c(seq(0,6,1)),c("count","uni","bi","tri")))

# read in 1-gram information
unigramDF<-read.csv("unigramDF.csv")[,-1]
unigramLength<-length(unigramDF$Uni)
unigramDF$Uni<-as.character(unigramDF$Uni)
unigramDF$counts<-as.numeric(unigramDF$counts)
uni.freqfreq<-data.frame(Uni=table(unigramDF$counts))

# Calculate probability of unseen unigram as per Jurafsky and Martin
GTCount[1,2] <- uni.freqfreq[1,2]
GTCount[2:7,2] <- uni.freqfreq[1:6,2]

kFactor <- 6*GTCount[7,2]/GTCount[2,2] # for k = 5
for (c in 0:5){
     num<-((c+1)*GTCount[c+2,2]/GTCount[c+1,2])-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,2]<-num/den
}
rm(unigramDF,uni.freqfreq)

# read in 2-gram information
twogramDF<-read.csv("twogramDF.csv")[,-1]
bigramLength<-length(twogramDF$Bi)
twogramDF$Bi<-as.character(twogramDF$Bi)
twogramDF$counts<-as.numeric(twogramDF$counts)
twogramDF$Uni<-sub(" .*","",twogramDF$Bi)

bi.freqfreq<-data.frame(Bi=table(twogramDF$counts))

# Calculate probability of unseen bigram as per Jurafsky and Martin
GTCount[1,3] <- unigramLength^2 - bigramLength
GTCount[2:7,3] <- bi.freqfreq[1:6,2]

kFactor <- 6*GTCount[7,3]/GTCount[2,3] # for k = 5
for (c in 0:5){
     num<-(c+1)*GTCount[c+2,3]/GTCount[c+1,3]-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,3]<-num/den
}
rm(twogramDF,bi.freqfreq)

# read in 3-gram information
threegramDF<-read.csv("threegramDF.csv")[,-1]
singleTri<-read.csv("singleTriGrams.csv")
singleTriLength<-length(singleTri$Tri)
trigramLength<-length(threegramDF$Tri)+singleTriLength
rm(singleTri)
threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram)
rm(bigram)
threegramDF$w3<-sub(".* ","",threegramDF$Tri)

tri.freqfreq <- data.frame(Tri=table(threegramDF$counts))
# Calculate probability of unseen trigram as per Jurafsky and Martin
GTCount[1,4] <- unigramLength^3 - trigramLength
GTCount[2,4] <- singleTriLength
GTCount[3:7,4] <- tri.freqfreq[1:5,2]

kFactor <- 6*GTCount[7,4]/GTCount[2,4] # for k = 5
for (c in 0:5){
     num<-(c+1)*GTCount[c+2,4]/GTCount[c+1,4]-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,4]<-num/den
}
write.csv(GTCount,"GTCount.csv")
rm(threegramDF,tri.freqfreq)

rm(list=ls())
```

*Table 4: Good Turing Smoothing matrix for predictive model*
  
 count |   uni    |       bi    |	   tri
----- | -------- | -------- | --------
  0   |   1.28   | 4.74E-05 |	7.81E-10
  1	 |   0.30   |   0.24   |	  0.17
  2	 |   1.32   |	 1.15   |	  0.22
  3	 |   2.39   |	 2.13   |	  3.44
  4   |	3.36   |	 3.13   |	  3.21
  5   |   4.30   |   4.15   |	  4.29 

The first prototype of the predictive model was run against test data. At this point, Katz back off approaches had been researched but were not implemented in the algorithms. The basic flow of the user interface with the algorithm:

* the user inputs two words (a 2-gram) to the `predict` function – with the goal of see ideas of a third word
* the 2-gram is searched for in the trigram model look-up table
* if that 2-gram does not exist, a simple message appears that the word did not exist in the look-up table (in prototype only)
* if the bigram is found, a subset of all 3-grams starting with that 2-gram are generated
* the algorithm next uses the Good-Turing Smoothing model to update the count for each element of the subset where (K:k < 5)
* the probability of the resulting trigram options is calculated by dividing the count of the trigram by the count of the bigram

The algorithm allowed for an accumulation of possible third words within 10 clusters (fixed during prototyping). During early runs of the algorithm with very a small corpus, there were many times when the probability of the third most likely word had up to 14 different options. As the corpus used in the model grew, the stratification of these probabilities was expected to become more granular. Allowing the user to see an adjustable number of clusters would be a value proposition for the user.

Having created this first predictive model it was run against an exercise provided by the instructors and additional test sets. The early predictive model - using a small corpus of 30,000 articles and documents - resulted in an accuracy of about 20 percent.

**Creative exploration**

Having a prototype allows for creativity and practicality to merge. First, the corpus used was increased to the full 60 percent training set of blog posts. Very quickly – and unfortunately – it was clear the preprocessing to develop the N-Gram matrices was not scalable. In other words, the lengths the number of observations (rows) vastly exceeded the physical RAM capabilities of 4 GB. This required an improvement in the approach to the algorithm to improve the basic preprocessing. Although, it was believed that the prediction algorithm will run as efficiently once the N-Gram models are brought into proper size. Revisions to the prototype data cleaning were implemented to: include end-of-sentence <EOS> and number <NUM> tags, conversion of various ASCII codes to appropriate language wordforms, remove punctuation except apostrophe and <> symbols in <EOS> and <NUM>, and remove web URLS. The full code of all cleaning processes is visible in <a href="#appendix-b---reading-in-the-data-and-cleaning">Appendix B - Reading in the Data and Cleaning</a>. These adaptations had the desired effect. The 60% training blog set could be fully processed within R. 

Another very important change was that in developing the trigram table processing it exceeded the physical RAM memory. The large number of single trigrams was removed from the dataframe. Although the results of this were seen in Figures 1 and 2, the singletons were not removed until after prototyping. Upon inspection, most of the singletons were very rare and odd combinations of words. The algorithm for preprocessing was adjusted to find all the single trigrams frequencies and remove them from the table. This process was not straightforward. One had to keep RAM limitations in mind. Therefore, it was accomplished in a series of iterative chunks – taking a subset of 10,000 documents at a time, processing them, and then using the `aggregate`, `merge`, and `rbind` functions of R. For the 60 percent training corpus, this required 54 For Loops and generated 54 individual data frames. 

Those 54 dataframes were then slowly condensed through the `merge` function to accumulate all of the counts so as to not eliminate a single trigram if it appeared in one data frame when it actually could appear and others. Once all the data frames were consolidated - and consolidated - and consolidated again, the total number of single trigrams amounted to over 15,000,000 trigrams. The final dataframe of trigrams for the model ended up being 1,464,904 observations. Reducing the dataframe increased speed and scalability. As for accuracy, this update model was run against the test sets and it increased accuracy to approximately 40 percent.

One last modification is worth discussing. The size of the three dataframes was a concern when loading it into a web-based environment. The initial and revised model used a design relying on a two-gram and a three-gram data frame. This amounted to 170.8 MB of data. Some experimentation was conducted to discern the effect of adding end of sentence markers <EOS>. This had a significant result and reduced the table sizes by 20 – 30 percent in terms of overall numbers of observations. An idea then emerged to eliminate the two-gram table from the model. Its purpose was a look-up table to determine if there were any 3-grams in the model. It was realized that the three-gram model already contained all the 2-grams and 1-grams. A simple `grep` command allowed the dataframe to expand on the server side to include the bigram and the unigram. The Katz back method was added into the algorithm to compensate for the elimination of single trigrams. This eliminated the earlier error messages that "no options could be provided" to the user. All told, these enhancements reduces the web-based model to a relies on a 42 MB CSV file that is scalable on all laptops, desktops, and mobile platforms tested. Previous coursework using the Shiny.io utility has successfully manipulated input data sets of that size and magnitude. 

The final prediction model is available in <a href="#appendix-e---prediction-model">Appendix E - Prediction Model</a>.

``` {r 6-predictingngramModel, echo=FALSE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
########
## This code relies on two .CSV datasets built using the other modules:
##    *GTCount.csv - contains the Good-Turing Smoothing parameters
##    * threegramDF.csv - contains all non-singleton trigrams and counts
########
options(digits=4)

## Read in prepared .CSV datasets
threegramDF<-read.csv("threegramDF.csv")[,-1]
threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram) # sets up the 2-gram lookup index
rm(bigram)
threegramDF$Uni<-sub(".* ","",threegramDF$Bi)
threegramDF$w3<-sub(".* ","",threegramDF$Tri) # provides list of output word

# read in Good-Turing Smoothing table
GTCount<-read.csv("GTCount.csv")[,-1]

##### Supplemental Function Calls #####
cleanInput <-function(word) {
##
## This function call takes in a user passes parameter and does basic
## preprocessing such as remove punctuation (retaining apostrophes),
## remove and preceding spaces, and remove multiple spaces in phrase.
##
## It returns the processed user input
##
    word <- tolower(word) #ensure input is in lower case
    word <- gsub("[^[:alnum:][:space:]\']", "",word)
    word <- gsub("^[ ]{1,10}","",word)
    word <- gsub("[ ]{2,10}"," ",word)
    return(word)
}

buildTable <-function(input,subTri,cluster) {
##
## This function takes in the cleaned input phrase and a subset of all
## seen tri-grams from threegramDF.csv based on user input. It also takes in
## a user defined number for the clusters they want displayed
##
## It returns the list of tri-grams for visual and tabular display
##
    diff = 0 # initialize counter - compares diff in probabilities of rows
    row.use=1 # initialize counter - counts total rows to subset from DF
    size <- dim(subTri)[1]
    if (size == 1){        # if the total trigrams found equals one
        useTri <- subTri   # simply use the one phrase and return as output
        return(useTri)
    } else {               # else build table of outputs based on cluster input
        remain = size - 1  # counter to work through list
        while (diff < cluster && remain >0) {
            if (subTri[row.use,2] - subTri[row.use+1,2] > .00001) diff=diff + 1
            row.use <-row.use + 1
            remain <- remain - 1    # calculates when end-of-list is reached
        }
        if (remain == 0) {
            useTri <- subTri[1:row.use,]  # when list is fully used
            return(useTri)
        } else {
            useTri <- subTri[1:row.use-1,]  # if max is reached first
            return(useTri)
        }
    }
}

###### Main Prediction Function #######

predict <-function(input,cluster=7){
## This function takes as input two words (a bigram) and uses that to enter
## lookup tables to find the highest probability trigrams that result.
## It emplolys Good-Turing Smoothing and Katz back off when conditions would
## suggest their use.
##
## Within the function, it produces a dotchart based on a default of 7 clusters
## The function returns a data frame contain the top predicted word endings
##
    Katz = FALSE # initialize Katz back off flag to FALSE
    gt = FALSE # initialize Good-Turing smoothing to FALSE
    input <- cleanInput(input)
    inputSize<-length(strsplit(input, " ")[[1]])
    if (inputSize != 2) stop("Please input exactly two words.\n",
        "Don't forget adding the space.")    # error handling
    nCount <- sum(threegramDF[which(threegramDF$Bi==input),2])
    if (nCount == 0) {     # bicount=0 therefore use Katz backoff
        Katz = TRUE
        input <- gsub(".* ","",input)    # isolates w2 as unigram
        nCount <- sum(threegramDF[which(threegramDF$Uni==input),2])
        if (nCount == 0) stop("This phrase is very unique.\n", 
            "I can't seem to find it.")     # error handling

        # Subset all recorded 2-grams that begin with unigram
        seekTri<-grepl(paste("^",input,"$",sep=""),threegramDF$Uni)
        subTri<-threegramDF[seekTri,] #subset relevant outputs
        # aggregation is key here because otherwise can provide
        # multiple output words as the front of bigrams was removed
        subTri<-aggregate(subTri$counts,list(subTri$w3),sum)
        names(subTri)<-c("w3","counts")
        subTri<-subTri[order(subTri$counts,decreasing=T),]
        useTri <- buildTable (input,subTri,cluster)
        for (i in 1:length(useTri$counts)) {
            count = useTri[i,2]
            if(count<=5) {     # employs the Good-Turing Smoothing
                useTri[i,2]<-GTCount[count+1,2]
                gt = TRUE
            }
        }
    } else {

        # Subset all recorded 3-grams that begin with bigram
        seekTri<-grepl(paste("^",input,"$",sep=""),threegramDF$Bi)
        subTri<-threegramDF[seekTri,] #subset relevant 3-grams
        subTri<-aggregate(subTri$counts,list(subTri$w3),sum)
        names(subTri)<-c("w3","counts")
        subTri<-subTri[order(subTri$counts,decreasing=T),]
        useTri <- buildTable (input,subTri,cluster)
        for (i in 1:length(useTri$counts)) {
            count = useTri[i,2]
            if(count<=5) {     # employs the Good-Turing Smoothing
                useTri[i,2]<-GTCount[count+1,4]
                gt = TRUE
            }
        }
    }
   
    options(digits = 4)

    ## generates data frame of tabular outputs for user review
    predictWord <- data.frame(Word=useTri$w3,
        probability=(useTri$counts/nCount)*100, stringsAsFactors=FALSE) 
    
    ## generates dotchart to visualize possible options, must invert order
    plot<-predictWord[order(predictWord$probability),] #order for lowest to highest
    dotchart(plot$probability,labels=plot$Word, pch=19, color="blue",
         xlab=paste("Probability (in %) of top",cluster,"clusters"),
         main=paste("N-Grams Starting with: \"",toupper(input),"\""))
    
    ## informative phrases as to what the user input was
    print(paste("Words completing N-Gram starting with: ",toupper(input)))
    if(Katz==TRUE){
        print("*Katz back off helped find options by reducing 2-gram to a 1-gram")
    }    
    if(gt == TRUE){
        print("*Good-Turing techniques were used in the solution")
    }
    
    return(predictWord)
}
```

# Reproducible documentation

All of the data processing, analysis, and results are fully reproducible by executing the code in the appendices. Alternatively, an R markdown file is freely available at [https://github.com/jgendron/datasciencecoursera](https://github.com/jgendron/datasciencecoursera/blob/master/NLP-A%20Model%20to%20Predict%20Word%20Sequences.Rmd). Those interested in collaborating on this or similar projects may contact the author at [LinkedIn: jaygendron](http://www.linkedin.com/in/jaygendron/).

# Results - Creating a data product

All told, these enhancements reduces the web-based model to a relies on a 42 MB CSV file that is scalable on all laptops, desktops, and mobile platforms tested. Previous coursework using the Shiny.io utility has successfully manipulated input data sets of that size and magnitude. Once loaded, the methods used to access the data frame allow the model to load in less than two minutes and subsequent queries of the model provide near instantaneous results to the user after they provide two words as input. They are presented with a list of possible end words to the complete the phrase and tools to adjust the amount of information they see. The interactive web-based model is available at the Shiny.io web site under the link [https://jgendron.shinyapps.io/predictngram/](https://jgendron.shinyapps.io/predictngram/).

# Conclusions

In this analysis, we determined common relationships between the various sub-corpus elements, the vocabulary, and word types, and their impacts on predictive power. Using information learned earlier during exploratory data analysis, the algorithms were refined to generate look-up tables that balanced depth of information with speed of processing. There was a subset of the corpus (specifically, 60 percent of the blog corpus) that produced an optimized balance between these competing properties. Through literature review, the Good-Turing approach was employed to developed the tables of counts which enabled prediction. The Katz Back off approach was implemented in the event a particular three-word phrase was not found in the look-up table. This analysis suggests that a predictive model can be built, but it is most useful for predicting common word stems as opposed to highly specialized language needs of a user.

# Acknowledgements
First, I would like to very much like to thank my wife - my partner - for all her understanding and support. Not only during this Capstone course, but through the entire sequence of the Data Science Specialization. She is *amazing* and for that I am eternally grateful. Thanks also to Dr. Thomas Bock for his explanation of key some mathematical concepts. Lastly, I would be remiss if I did not acknowledge the power of collaborative group dynamics. Many, many ideas and solutions are rooted in part to the generous comments from my colleagues on the Capstone Discussion Forum. I would like to thank and acknowledge the entire beta class who provide sage advice and support in that collaborative space.

# References

Buchta, C., Hornik, K, Feinerer, I., & Meyer, D. (2014, June 11). tau: Text analysis utilities (Version 0.0-18) [Software0. Available from [http://cran.r-project.org/web/packages/tau/index.html]( http://cran.r-project.org/web/packages/tau/index.html).

Feinerer, I. Hornik, K., & Artifex Software. (2014, June 11). tm: Text mining package (Version 0.6) [Software]. Available from [http://cran.r-project.org/web/packages/tm/index.html]( http://cran.r-project.org/web/packages/tm/index.html).

Feinerer, I., Hornik, K., & Meyer, D. (2008, March). Text mining infrastructure in R. *Journal of Statistical Computing*, 25(5). Retrieved from [http://www.jstatsoft.org/v25/i05/paper](http://www.jstatsoft.org/v25/i05/paper).

Jurafsky, D. & Martin, J.H. (2000). *Speech and language processing: An introduction to natural language processing, computational linguistics and speech recognition*. Englewood Cliffs, NJ: Prentice Hall.

R Core Team (2014, July 10). R: A language and environment for statistical computing. R Foundation for Statistical Computing (Version 3.1.1) [Software]. Vienna, Austria. Available from [http://www.R-project.org/]( http://www.R-project.org/).

Richards, B. (1987). Type/token ratios:What do they really tell us? *Journal of Child Language*, 14, pp. 201209. Doi: 10:1017/S0305000900012885. Retrieved from [http://hfroehlich.files.wordpress.com/2013/05/s0305000900012885a.pdf](http://hfroehlich.files.wordpress.com/2013/05/s0305000900012885a.pdf).

Turing, A.M. (1950). Computing machinery and intelligence. *Mind*, 59, 433-460. Retrieved from [http://www.loebner.net/Prizef/TuringArticle.html](http://www.loebner.net/Prizef/TuringArticle.html).

******
# Appendices

## Appendix A - Generating Training and Testing Sets

``` {r A-createDataSubsets, echo=TRUE, eval=FALSE, warning=FALSE,error=FALSE,cache=TRUE}

##################################
##################################
## For RMD purposes, please note that the entire body of code is place within 
## the document to allow for runs of graphics. You will notice that most front
## "Chunks" are set to echo=FALSE and eval=FALSE. This is done to enable 
## generation of the PDF report
##################################
##################################


##
## Create data subsets from original datasets provided
##

###
## Note: prior to running, create these three folders and extract the associated
## data into each - one corpus per folder:
# Folder Names: blogs, news, twitter
###

library(tm) # Use tm package
# Read in three English text data sets already extracted from .zip file
# Used virtual corpus for this interim work
tweet <- VCorpus(DirSource("twitter", encoding = "UTF-8"), 
                 readerControl=list(language="en"))
blog <- VCorpus(DirSource("blogs", encoding = "UTF-8"),
                readerControl=list(language="en"))
news <- VCorpus(DirSource("news", encoding = "UTF-8"),
                readerControl=list(language="en"))
########
## Approach to split corpora into train, devtest, and test sets
########

# Permute entire tweet group to randomize order
set.seed(0330)
perm.tweet <- sample(tweet[[2]][[1]], length(tweet[[2]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.tweet))
twtTrain <- perm.tweet[1:TR]
remain<-perm.tweet[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
twtDevtest <- remain[1:DEV]
twtTest <- remain[-(1:DEV)]
write(twtTrain,"twtTrain.txt")
write(twtDevtest,"twtDevtest.txt")
write(twtTest,"twtTest.txt")
rm(list = ls()) #clean environment

# Permute entire blog group to randomize order                     
set.seed(0330)
perm.blog <- sample(blog[[1]][[1]], length(blog[[1]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.blog))
blogTrain <- perm.blog[1:TR]
remain<-perm.blog[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
blogDevtest <- remain[1:DEV]
blogTest <- remain[-(1:DEV)]
write(blogTrain,"blogTrain.txt")
write(blogDevtest,"blogDevtest.txt")
write(blogTest,"blogTest.txt")
rm(list = ls()) #clean environment

# Permute entire news group to randomize order   
set.seed(0330)
perm.news <- sample(news[[1]][[1]], length(news[[1]][[1]]))
# sets the ratio of training set at 60%
TR <- round(0.6*length(perm.news))
newsTrain <- perm.news[1:TR]
remain<-perm.news[-(1:TR)]
# splits remaining dataset in half for devtest and test set
DEV <- round(0.5*(length(remain)))
newsDevtest <- remain[1:DEV]
newsTest <- remain[-(1:DEV)]
write(newsTrain,"newsTrain.txt")
write(newsDevtest,"newsDevtest.txt")
write(newsTest,"newsTest.txt")
rm(list = ls()) #clean environment
```

******
## Appendix B - Reading in the Data and Cleaning

``` {r B-readTraininSettoClean, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Read in dataset and clean for use. The DirSource "training" is a pointer to 
## the location of the training corpus created
##

###
## Note: after creating the blog training set "blogTrain.txt from the previous
## code, add a folder "training" and include "blogTrain.txt" in it.
## Also and create folder "mod" to hold in-process cleaning data
###

library(tm)   # package used to read dataset
library(filehash) # package used to update/maintain permanent corpus database
myCorpus <- PCorpus(DirSource("training", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="myCorpus.db", dbType="DB1"))

##
## Cleaning steps refined  over time
## Accomplished in three loops due to RAM limitations
##

# to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower));dbInit("myCorpus.db")

for (j in seq(myCorpus)) {
     # first two separate hyphenated and slashed words
     myCorpus[[j]][[1]] <-gsub("-", " ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("/", " ", myCorpus[[j]][[1]])
     # converts symbol <> into an apostrophe
     myCorpus[[j]][[1]] <-gsub("<>", "\\'", myCorpus[[j]][[1]])
     print("3 of 18 transformations complete") #provides progress to user
     # these three create end of sentence markers <EOS>
     myCorpus[[j]][[1]] <-gsub("\\. |\\.$","  <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\? |\\?$","  <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\! |\\!$","  <EOS> ", myCorpus[[j]][[1]])
     print("6 of 18 transformations complete") #provides progress to user
}
write(myCorpus[[1]][[1]],"./mod/blogTrain.txt") # writes corpus to permanent disc
# reads back in corpus...this method reduced 908MB corpus to 137MB
myCorpus <- PCorpus(DirSource("mod", 
     encoding="UTF-8",mode="text"),
          dbControl=list(dbName="halfCorpus.db", dbType="DB1"))

for (j in seq(myCorpus)) {
     # tranforms various ASCII codes to appropriate language 
     myCorpus[[j]][[1]] <-gsub("<85>"," <EOS> ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("<92>","'", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub("\\&", " and ", myCorpus[[j]][[1]])
     print("9 of 18 transformations complete") #provides progress to user
     # removes all punctuation except apostrophe and <> symbols in <EOS>
     myCorpus[[j]][[1]] <-gsub("[^[:alnum:][:space:]\'<>]", " ",
          myCorpus[[j]][[1]])
     # removes web site URLs
     myCorpus[[j]][[1]] <-gsub(" www(.+) ", " ", myCorpus[[j]][[1]])
     # removes all single letters except "a" and "i"
     myCorpus[[j]][[1]] <-gsub(" [b-hj-z] "," ", myCorpus[[j]][[1]])
     print("12 of 18 transformations complete") #provides progress to user
}

write(myCorpus[[1]][[1]],"./mod/blogTrain.txt")  # writes corpus to disc
# reduced 937 MB corpus to 133 MB
myCorpus <- PCorpus(DirSource("mod",
     encoding="UTF-8",mode="text"),dbControl=list(dbName="lastCorpus.db",
          dbType="DB1"))

for (j in seq(myCorpus)) {
     # removes errant apostrohes introduced by transformations
     myCorpus[[j]][[1]] <-gsub(" ' "," ", myCorpus[[j]][[1]])        
     myCorpus[[j]][[1]] <-gsub("\\' ", " ", myCorpus[[j]][[1]])
     myCorpus[[j]][[1]] <-gsub(" ' ", " ", myCorpus[[j]][[1]])
     print("15 of 18 transformations complete") #provides progress to user
     # removes errant codes in < > brackets
     myCorpus[[j]][[1]] <-gsub("<[^EOS].+>"," ", myCorpus[[j]][[1]])
     # replaces numbers with a number marker <NUM> for context
     myCorpus[[j]][[1]] <-gsub("[0-9]+"," <NUM> ", myCorpus[[j]][[1]])
     # removes and errant <> brackets remaining
     myCorpus[[j]][[1]] <-gsub("<>"," ", myCorpus[[j]][[1]])
     print("18 of 18 transformations complete") #provides progress to user
}
# removes numbers and the tm package "dbInit" compresses data in RAM
myCorpus <- tm_map(myCorpus, removeNumbers);dbInit("lastCorpus.db")
# removes errant 's symbols not as contractions
myCorpus[[1]][[1]] <-gsub(" 's"," ", myCorpus[[1]][[1]])
# removes errant close brackets starting a word
myCorpus[[1]][[1]] <-gsub(">[a-z]"," ", myCorpus[[1]][[1]])

myCorpus <- tm_map(myCorpus, stripWhitespace);dbInit("lastCorpus.db")
# final corpus after all processing only 111 MB

# Writes final, processed corpus to disc for building n-grams
write(myCorpus[[1]][[1]],"./mod/blogTrain.txt") # write PCorpus to disc
# original training set 130 MB; clean training seet 108 MB
```

******
## Appendix C - Prototype Modeling - Building N-Gram Models

### C-1: Building the 1-Gram (unigram) Model

``` {r C1-buildOneGramModel, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Code uses blogTrain.txt to generate list of all 1-gram (unigrams)
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="aggCorpus.db", dbType="DB1"))

library(tau) #package to create N-Grams
ngram <- function(n) {
     textcnt(CORP, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]]) #pulls out the text element from the list myCorpus

oneGram <- ngram(1)
unigramDF<-data.frame(Uni = names(oneGram), counts = unclass(oneGram))
rm(oneGram)
unigramDF$Uni<-as.character(unigramDF$Uni)
unigramDF$counts<-as.numeric(unigramDF$counts)
# removes the "words" <eos> and <num> from unigram table
unigramDF <- unigramDF[which(unigramDF$Uni!="<eos>"),]
unigramDF <- unigramDF[which(unigramDF$Uni!="<num>"),]

lengthUni<-length(unigramDF$Uni) #253,921 unigrams

# Builds frequency of frequency table for Good-Turing smoothing
uni.freqfreq<-data.frame(Uni=table(unigramDF$counts))

write.csv(unigramDF,"unigramDF.csv") #2,620 frequencies
write.csv(uni.freqfreq,"uni.freqfreq.csv")
rm(unigramDF,uni.freqfreq,CORP,myCorpus)
```


### C-2: Building the 2-Gram (bigram) Model
``` {r C2-buildTwoGramModel, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Code uses blogTrain.txt to generate list of all 2-grams (bigrams)
## Specifically designed to break large corpus into chunks to deal
## with limitations imposed by RAM in R
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="biCorpus.db", dbType="DB1"))
library(tau) #package to create N-Grams
ngram <- function(n) {
     textcnt(CORPport, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]]) #pulls out the text element from the list myCorpus
rm(myCorpus) #reduce RAM load
# determine number of loop runs to process 10,000 docs per run
step<-trunc(length(CORP)/10000)
remain<- length(CORP)-(step * 10000)
# first deal with the remainder
CORPport<-CORP[1:remain]
# invokes tau package to build the bigrams
twoGram <- ngram(2)
names(twoGram) <- gsub("^\'","",names(twoGram))        
twogramDF<-data.frame(Bi = names(twoGram), counts = unclass(twoGram))
names(twogramDF)<-c("Bi","counts")
## removes the "words" <eos> and <num> from unigram table
eosTag<-grepl("<eos>",twogramDF$Bi)
twogramDF<-twogramDF[!eosTag,]
numTag<-grepl("<num>",twogramDF$Bi)
twogramDF<-twogramDF[!numTag,]
# writes first of N:n=step dataframes
write.csv (twogramDF,"twoGramDF1.csv")
CORP<-CORP[-(1:remain)] #remove already processed docs from corpus

for (i in 1:(step-1)) {  # loop to process steps of 10000 docs
     CORPport<-CORP[1:10000]
     twoGram <- ngram(2)
     names(twoGram) <- gsub("^\'","",names(twoGram))        
     tmptwogramDF<-data.frame(Bi = names(twoGram), counts = unclass(twoGram))
     print(paste("Iteration",i,"of",step)) #provide user progress
     name<-paste("twoGramDF",(i+1),".csv",sep="")
     ## removes the "words" <eos> and <num> from unigram table
     eosTag<-grepl("<eos>",tmptwogramDF$Bi)
     tmptwogramDF<-tmptwogramDF[!eosTag,]
     numTag<-grepl("<num>",tmptwogramDF$Bi)
     tmptwogramDF<-tmptwogramDF[!numTag,]
     write.csv (tmptwogramDF,name)
     twogramDF<-rbind(twogramDF,tmptwogramDF)
     twogramDF<-aggregate(twogramDF$counts,list(Bi=twogramDF$Bi),sum)
     names(twogramDF)<-c("Bi","counts")
     CORP<-CORP[-(1:10000)]
}
rm(tmptwogramDF,numTag,eosTag,ngram,CORPport,twoGram)
rm(remain,step,name,i,CORP)
twogramDF$Bi<-as.character(twogramDF$Bi)
twogramDF$counts<-as.numeric(twogramDF$counts)
twogramDF$Uni<-sub(" .*","",twogramDF$Bi)
# Builds frequency of frequency table for Good-Turing smoothing
bi.freqfreq<-data.frame(Bi=table(twogramDF$counts))
write.csv(twogramDF,"twogramDF.csv")
write.csv(bi.freqfreq,"bi-freqfreq.csv")
rm(bi.freqfreq,twogramDF)
```

### C-3: Building the 3-Gram (trigram) Model
``` {r C3-buildThreeGramModel, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Code uses blogTrain.txt to generate list of all 3-grams (trigrams)
## Specifically designed to break large corpus into chunks to deal
## with limitations imposed by RAM in R
##
library(tm)   # package used to read dataset
library(filehash)
myCorpus <- PCorpus(DirSource("mod", encoding="UTF-8",mode="text"),
                    dbControl=list(dbName="triCorpus.db", dbType="DB1"))
library(tau)
ngram <- function(n) {
     textcnt(CORPport, method = "string",n=as.integer(n),
             split = "[[:space:][:digit:]]+",decreasing=T)
}
CORP<-c(myCorpus[[1]][[1]])
rm(myCorpus) #reduce RAM load
# determine number of loop runs to process 10,000 docs per run
step<-trunc(length(CORP)/10000)
for (i in 1:(step)) {
     CORPport<-CORP[1:10000]
     # invokes tau package to build the trigrams
     threeGram <- ngram(3)
     names(threeGram) <- gsub("^\'","",names(threeGram))
     threegramDF<-data.frame(Tri = names(threeGram), counts = unclass(threeGram))
     print(paste("Iteration",i,sep=" "))
     name<-paste("threeGramDF",i,".csv",sep="")
     ## removes the "words" <eos> and <num> from trigram table
     eosTag<-grepl("<eos>",threegramDF$Tri)
     threegramDF<-threegramDF[!eosTag,]
     numTag<-grepl("<num>",threegramDF$Tri)
     threegramDF<-threegramDF[!numTag,]
     # writes first of N:n=step dataframes
     write.csv (threegramDF,name)
     CORP<-CORP[-(1:10000)]
}
CORPport<-CORP
# repeats above loop for the remaining documents (partial step)
threeGram <- ngram(3)
names(threeGram) <- gsub("^\'","",names(threeGram))
threegramDF<-data.frame(Tri = names(threeGram), counts = unclass(threeGram))
names(threegramDF)<-c("Tri","counts")
name<-paste("threeGramDF",i+1,".csv",sep="")
eosTag<-grepl("<eos>",threegramDF$Tri)
threegramDF<-threegramDF[!eosTag,]
numTag<-grepl("<num>",threegramDF$Tri)
threegramDF<-threegramDF[!numTag,]
write.csv (threegramDF,name)

rm(threegramDF, CORP,CORPport,threeGram,i,name)

#####
## Begin aggregating dataframes in groups to develop one trigram dataframe
#####

for (k in seq(1,46,5)) {
     key<-paste("threeGramDF",k,".csv",sep="")
     print(k)
     threeGramDF <-read.csv(file = key)
     threeGramDF<-threeGramDF[,-1]
     for (m in 1:4) {
          name<-paste("threeGramDF",(k+m),".csv",sep="")
          temp <- read.csv(name)
          temp<-temp[,-1]
          threeGramDF <- merge(threeGramDF,temp, by.x="Tri",by.y="Tri", all=TRUE)
          print(k+m)
          rm(temp)
     }
     out<-paste("mergedTri",k,".csv",sep="")
     counts<-threeGramDF[,2:ncol(threeGramDF)]
     threeGramDF$counts<-rowSums(counts,na.rm=TRUE)
     threeGramDF<-threeGramDF[,-(2:(ncol(threeGramDF)-1))]
     write.csv(threeGramDF,out)
     rm(threeGramDF,counts)
}

k=k+5

key<-paste("threeGramDF",k,".csv",sep="")
print(k)
threeGramDF <-read.csv(file = key)
threeGramDF<-threeGramDF[,-1]
for (m in 1:3) {
     name<-paste("threeGramDF",(k+m),".csv",sep="")
     temp <- read.csv(name)
     temp<-temp[,-1]
     threeGramDF <- merge(threeGramDF,temp, by.x="Tri",by.y="Tri", all=TRUE)
     print(k+m)
     rm(temp)
}
out<-paste("mergedTri",k,".csv",sep="")
counts<-threeGramDF[,2:ncol(threeGramDF)]
threeGramDF$counts<-rowSums(counts,na.rm=TRUE)
threeGramDF<-threeGramDF[,-(2:(ncol(threeGramDF)-1))]
write.csv(threeGramDF,out)
rm(threeGramDF,counts)

#####
## continue aggregating dataframes in groups to develop one trigram dataframe
#####

for (p in c(1,11,21,31,41)) {
     m1<-paste("mergedTri",p,".csv",sep="")
     m6<-paste("mergedTri",(p+5),".csv",sep="")
     print(m1)
     print(m6)
     m1<-read.csv(m1)[,-1]
     m6<-read.csv(m6)[,-1]
     super <- merge(m1,m6, by.x="Tri",by.y="Tri", all=TRUE)
     rm(m1,m6)
     counts<-super[,2:ncol(super)]
     super$counts<-rowSums(counts,na.rm=TRUE)
     super<-super[,-(2:(ncol(super)-1))]
     supOut<-paste("superTri",p,".csv",sep="")
     write.csv(super,supOut)
     rm(super,counts)
}        

superTri51<-read.csv("mergedTri51.csv")[,-1]
write.csv(superTri51,"superTri51.csv")
rm(superTri51)

#####
## Continue aggregating dataframes in groups to develop one trigram dataframe
#####

for (p in c(1,21,41)) {
     m1<-paste("superTri",p,".csv",sep="")
     m6<-paste("superTri",(p+10),".csv",sep="")
     print(m1)
     print(m6)
     m1<-read.csv(m1)[,-1]
     m6<-read.csv(m6)[,-1]
     super <- merge(m1,m6, by.x="Tri",by.y="Tri", all=TRUE)
     rm(m1,m6)
     counts<-super[,2:ncol(super)]
     super$counts<-rowSums(counts,na.rm=TRUE)
     super<-super[,-(2:(ncol(super)-1))]
     supOut<-paste("superTri2-",p,".csv",sep="")
     write.csv(super,supOut)
     rm(super,counts)
} 

#####
## First of final 2 aggregating processes to develop one trigram dataframe
#####

A<-read.csv("superTri2-1.csv")[,-1]
AtwoPlus<-A[which(A$counts>=2),]
Asingles<-A[which(A$counts==1),]
rm(A)

B<-read.csv("superTri2-21.csv")[,-1]
BtwoPlus<-B[which(B$counts>=2),]
Bsingles<-B[which(B$counts==1),]
rm(B)

# creates aggregated dataframes for singleton trigrams and all others
threegramDF <- merge(AtwoPlus,BtwoPlus, by.x="Tri",by.y="Tri", all=TRUE)
rm(AtwoPlus,BtwoPlus)
write.csv(threegramDF,"threegramDF.csv")
rm(threegramDF)
singles <-merge(Asingles,Bsingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(Asingles,Bsingles)
write.csv(singles,"interimABsingles.csv")

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri1.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
rm(singleBlend)

half<-trunc(length(singles$counts)/2)
half2<-length(singles$counts)-half

singlesFirstHalf<-singles[1:half,]
write.csv(singlesFirstHalf,"firstHalf.csv")
rm(singlesFirstHalf)
singlesSecondHalf<-singles[-(1:half),]
rm(singles)
write.csv(singlesSecondHalf,"secondHalf.csv")

#####
## Second of final 2 aggregating processes to develop one trigram dataframe
#####

C<-read.csv("superTri2-41.csv")[,-1]
CtwoPlus<-C[which(C$counts>=2),]
Csingles<-C[which(C$counts==1),]
rm(C)

threegramDF<-read.csv("threegramDF.csv")
threegramDF<-threegramDF[,-1]
threegramDF <- merge(threegramDF,CtwoPlus, by.x="Tri",by.y="Tri", all=TRUE)
rm(CtwoPlus)
counts<-threegramDF[,2:ncol(threegramDF)]
threegramDF$counts<-rowSums(counts,na.rm=TRUE)
threegramDF<-threegramDF[,-(2:(ncol(threegramDF)-1))]
rm(counts)
write.csv(threegramDF,"threegramDF.csv")
rm(threegramDF)

singles <-merge(singlesSecondHalf,Csingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(singlesSecondHalf)

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri2.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
write.csv(singles,"halfRealsingles2.csv")
rm(singleBlend,singles)

firstHalf<-read.csv("firstHalf.csv")
firstHalf<-firstHalf[,-1]
singles <-merge(firstHalf,Csingles, by.x="Tri",by.y="Tri", all=TRUE)
rm(Csingles,firstHalf)

counts<-singles[,2:ncol(singles)]
singles$counts<-rowSums(counts,na.rm=TRUE)
singles<-singles[,-(2:(ncol(singles)-1))]
rm(counts)
singleBlend<-which(singles$counts>1)
singlesAdd2Tri<-singles[singleBlend,]
write.csv(singlesAdd2Tri,"add2Tri3.csv")
rm(singlesAdd2Tri)
singles<-singles[-singleBlend,]
rm(singleBlend)
write.csv(singles,"halfRealSingles1.csv")
rm(singles)

####
## Bind all work together
####

threegramDF<-read.csv("threegramDF.csv")
threegramDF<-threegramDF[,-1]
one<-read.csv("add2Tri1.csv")
one<-one[-1]
threegramDF<-rbind(threegramDF,one)
rm(one)
two<-read.csv("add2Tri2.csv")
two<-two[-1]
threegramDF<-rbind(threegramDF,two)
rm(two)
three<-read.csv("add2Tri3.csv")
three<-three[-1]
threegramDF<-rbind(threegramDF,three)
rm(three)
### Resultant trigram dataframe
write.csv(threegramDF,"threegramDF.csv")

threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram)
rm(bigram)
threegramDF$Uni<-sub(".* ","",threegramDF$Bi)
threegramDF$w3<-sub(".* ","",threegramDF$Tri) # provides the suggested word
# Builds frequency of frequency table for Good-Turing smoothing
tri.freqfreq <- data.frame(Tri=table(threegramDF$counts))
write.csv(threegramDF[,-1],"threegramDF.csv")
write.csv(tri.freqfreq,"tri-freqfreq.csv")
rm(tri.freqfreq,threegramDF) 

####
## create dataframe of singleton trigrams
####
S1<-read.csv("halfRealSingles1.csv")
S1<-S1[,-1]
S2<-read.csv("halfRealSingles2.csv")
S2<-S2[,-1]
S1<-rbind(S1,S2)
write.csv(S1,"singleTriGrams.csv")
rm(S1,S2)
```

******
## Appendix D - Implementing Good-Turing Smoothing

``` {r D-generateGTmatrix, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
##
## Generates Good-Turing matrix for prediction model
##
# build empty matrix
GTCount<-matrix(c(seq(0,6,1),rep(0,21)),nrow=7,ncol=4,
                dimnames = list(c(seq(0,6,1)),c("count","uni","bi","tri")))

# read in 1-gram information
unigramDF<-read.csv("unigramDF.csv")[,-1]
unigramLength<-length(unigramDF$Uni)
unigramDF$Uni<-as.character(unigramDF$Uni)
unigramDF$counts<-as.numeric(unigramDF$counts)
uni.freqfreq<-data.frame(Uni=table(unigramDF$counts))

# Calculate probability of unseen unigram as per Jurafsky and Martin
GTCount[1,2] <- uni.freqfreq[1,2]
GTCount[2:7,2] <- uni.freqfreq[1:6,2]

kFactor <- 6*GTCount[7,2]/GTCount[2,2] # for k = 5
for (c in 0:5){
     num<-((c+1)*GTCount[c+2,2]/GTCount[c+1,2])-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,2]<-num/den
}
rm(unigramDF,uni.freqfreq)

# read in 2-gram information
twogramDF<-read.csv("twogramDF.csv")[,-1]
bigramLength<-length(twogramDF$Bi)
twogramDF$Bi<-as.character(twogramDF$Bi)
twogramDF$counts<-as.numeric(twogramDF$counts)
twogramDF$Uni<-sub(" .*","",twogramDF$Bi)

bi.freqfreq<-data.frame(Bi=table(twogramDF$counts))

# Calculate probability of unseen bigram as per Jurafsky and Martin
GTCount[1,3] <- unigramLength^2 - bigramLength
GTCount[2:7,3] <- bi.freqfreq[1:6,2]

kFactor <- 6*GTCount[7,3]/GTCount[2,3] # for k = 5
for (c in 0:5){
     num<-(c+1)*GTCount[c+2,3]/GTCount[c+1,3]-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,3]<-num/den
}
rm(twogramDF,bi.freqfreq)

# read in 3-gram information
threegramDF<-read.csv("threegramDF.csv")[,-1]
singleTri<-read.csv("singleTriGrams.csv")
singleTriLength<-length(singleTri$Tri)
trigramLength<-length(threegramDF$Tri)+singleTriLength
rm(singleTri)
threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram)
rm(bigram)
threegramDF$w3<-sub(".* ","",threegramDF$Tri)

tri.freqfreq <- data.frame(Tri=table(threegramDF$counts))
# Calculate probability of unseen trigram as per Jurafsky and Martin
GTCount[1,4] <- unigramLength^3 - trigramLength
GTCount[2,4] <- singleTriLength
GTCount[3:7,4] <- tri.freqfreq[1:5,2]

kFactor <- 6*GTCount[7,4]/GTCount[2,4] # for k = 5
for (c in 0:5){
     num<-(c+1)*GTCount[c+2,4]/GTCount[c+1,4]-(c)*kFactor
     den<- 1-kFactor
     GTCount[c+1,4]<-num/den
}
write.csv(GTCount,"GTCount.csv")
rm(threegramDF,tri.freqfreq)

rm(list=ls())
```

******
## Appendix E - Prediction Model

``` {r E-predictingngramModel, echo=TRUE, eval=FALSE,warning=FALSE,error=FALSE,cache=TRUE}
########
## This code relies on two .CSV datasets built using the other modules:
##    *GTCount.csv - contains the Good-Turing Smoothing parameters
##    * threegramDF.csv - contains all non-singleton trigrams and counts
########
options(digits=4)

## Read in prepared .CSV datasets
threegramDF<-read.csv("threegramDF.csv")[,-1]
threegramDF$Tri<-as.character(threegramDF$Tri)
threegramDF$counts<-as.numeric(threegramDF$counts)
bigram<-sub(" ","@@@@",threegramDF$Tri)
bigram<-sub(" .*","",bigram)
threegramDF$Bi<-sub("@@@@"," ",bigram) # sets up the 2-gram lookup index
rm(bigram)
threegramDF$Uni<-sub(".* ","",threegramDF$Bi)
threegramDF$w3<-sub(".* ","",threegramDF$Tri) # provides list of output word

# read in Good-Turing Smoothing table
GTCount<-read.csv("GTCount.csv")[,-1]

##### Supplemental Function Calls #####
cleanInput <-function(word) {
##
## This function call takes in a user passes parameter and does basic
## preprocessing such as remove punctuation (retaining apostrophes),
## remove and preceding spaces, and remove multiple spaces in phrase.
##
## It returns the processed user input
##
    word <- tolower(word) #ensure input is in lower case
    word <- gsub("[^[:alnum:][:space:]\']", "",word)
    word <- gsub("^[ ]{1,10}","",word)
    word <- gsub("[ ]{2,10}"," ",word)
    return(word)
}

buildTable <-function(input,subTri,cluster) {
##
## This function takes in the cleaned input phrase and a subset of all
## seen tri-grams from threegramDF.csv based on user input. It also takes in
## a user defined number for the clusters they want displayed
##
## It returns the list of tri-grams for visual and tabular display
##
    diff = 0 # initialize counter - compares diff in probabilities of rows
    row.use=1 # initialize counter - counts total rows to subset from DF
    size <- dim(subTri)[1]
    if (size == 1){        # if the total trigrams found equals one
        useTri <- subTri   # simply use the one phrase and return as output
        return(useTri)
    } else {               # else build table of outputs based on cluster input
        remain = size - 1  # counter to work through list
        while (diff < cluster && remain >0) {
            if (subTri[row.use,2] - subTri[row.use+1,2] > .00001) diff=diff + 1
            row.use <-row.use + 1
            remain <- remain - 1    # calculates when end-of-list is reached
        }
        if (remain == 0) {
            useTri <- subTri[1:row.use,]  # when list is fully used
            return(useTri)
        } else {
            useTri <- subTri[1:row.use-1,]  # if max is reached first
            return(useTri)
        }
    }
}

###### Main Prediction Function #######

predict <-function(input,cluster=7){
## This function takes as input two words (a bigram) and uses that to enter
## lookup tables to find the highest probability trigrams that result.
## It emplolys Good-Turing Smoothing and Katz back off when conditions would
## suggest their use.
##
## Within the function, it produces a dotchart based on a default of 7 clusters
## The function returns a data frame contain the top predicted word endings
##
    Katz = FALSE # initialize Katz back off flag to FALSE
    gt = FALSE # initialize Good-Turing smoothing to FALSE
    input <- cleanInput(input)
    inputSize<-length(strsplit(input, " ")[[1]])
    if (inputSize != 2) stop("Please input exactly two words.\n",
        "Don't forget adding the space.")    # error handling
    nCount <- sum(threegramDF[which(threegramDF$Bi==input),2])
    if (nCount == 0) {     # bicount=0 therefore use Katz backoff
        Katz = TRUE
        input <- gsub(".* ","",input)    # isolates w2 as unigram
        nCount <- sum(threegramDF[which(threegramDF$Uni==input),2])
        if (nCount == 0) stop("This phrase is very unique.\n", 
            "I can't seem to find it.")     # error handling

        # Subset all recorded 2-grams that begin with unigram
        seekTri<-grepl(paste("^",input,"$",sep=""),threegramDF$Uni)
        subTri<-threegramDF[seekTri,] #subset relevant outputs
        # aggregation is key here because otherwise can provide
        # multiple output words as the front of bigrams was removed
        subTri<-aggregate(subTri$counts,list(subTri$w3),sum)
        names(subTri)<-c("w3","counts")
        subTri<-subTri[order(subTri$counts,decreasing=T),]
        useTri <- buildTable (input,subTri,cluster)
        for (i in 1:length(useTri$counts)) {
            count = useTri[i,2]
            if(count<=5) {     # employs the Good-Turing Smoothing
                useTri[i,2]<-GTCount[count+1,2]
                gt = TRUE
            }
        }
    } else {

        # Subset all recorded 3-grams that begin with bigram
        seekTri<-grepl(paste("^",input,"$",sep=""),threegramDF$Bi)
        subTri<-threegramDF[seekTri,] #subset relevant 3-grams
        subTri<-aggregate(subTri$counts,list(subTri$w3),sum)
        names(subTri)<-c("w3","counts")
        subTri<-subTri[order(subTri$counts,decreasing=T),]
        useTri <- buildTable (input,subTri,cluster)
        for (i in 1:length(useTri$counts)) {
            count = useTri[i,2]
            if(count<=5) {     # employs the Good-Turing Smoothing
                useTri[i,2]<-GTCount[count+1,4]
                gt = TRUE
            }
        }
    }
   
    options(digits = 4)

    ## generates data frame of tabular outputs for user review
    predictWord <- data.frame(Word=useTri$w3,
        probability=(useTri$counts/nCount)*100, stringsAsFactors=FALSE) 
    
    ## generates dotchart to visualize possible options, must invert order
    plot<-predictWord[order(predictWord$probability),] #order for lowest to highest
    dotchart(plot$probability,labels=plot$Word, pch=19, color="blue",
         xlab=paste("Probability (in %) of top",cluster,"clusters"),
         main=paste("N-Grams Starting with: \"",toupper(input),"\""))
    
    ## informative phrases as to what the user input was
    print(paste("Words completing N-Gram starting with: ",toupper(input)))
    if(Katz==TRUE){
        print("*Katz back off helped find options by reducing 2-gram to a 1-gram")
    }    
    if(gt == TRUE){
        print("*Good-Turing techniques were used in the solution")
    }
    
    return(predictWord)
}
```

******
## Appendix F - Glossary of Key Terms

**Good-Turing Discounting:** Termed Good-Turing smoothing, it is a technique to re-estimate probability mass to assign N-Grams with zero or low counts by discounting from those occurring more often."

**Katz back off:** Termed back off N-Gram modeling, it was developed in 1987 by Katz which predicts first based on non-zero, higher-order N-Grams and will "back off" to a lower-order N-Gram if there is zero evidence of the higher-order N-Gram.

**N-Gram:** A special type of wordform which looks *(N - n) words into the past and possesses the memory less properties of a Markov model. A 3-gram looks at the previous 2-gram to make the prediction.

**N-Gram cluster:** In calculating N-Grams, many probabilities are actually counts based on Good-Turing Discounting. A cluster is a group of words (sometimes a large number) having the same likelihood of appearing in the prediction model.

******
## Appendix G - Development Platform Information

``` {r G-sysInfo, echo=FALSE}
sessionInfo()
```
